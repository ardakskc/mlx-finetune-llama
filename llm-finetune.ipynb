{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get necessary libs\n",
    "import os\n",
    "import mlx_lm\n",
    "import torch\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Model Loading and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Login HuggingFace with API Key\n",
    "import huggingface_hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "MODEL_ID=\"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
    "\n",
    "huggingface_hub.login(HUGGING_FACE_API_KEY)\n",
    "# Download Pre-trained LLM\n",
    "huggingface_hub.snapshot_download(repo_id=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Connectivity for can we use model offline:\n",
    "# Step 1: Turn Off Wifi\n",
    "# Step 2: Run Cell and Test LLM Outputs\n",
    "# Step 3: Turn On Wifi and go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline,AutoTokenizer,AutoModelForCausalLM,AutoModelForSeq2SeqLM,BitsAndBytesConfig\n",
    "\n",
    "#Load model with 4-bit mode. This cause low memory usage.\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_use_double_quant=True) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,device_map=\"auto\",quantization_config=quantization_config)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_length=128)\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get output with Langchain Prompt Template\n",
    "prompt = langchain.PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"Can you answer this question: '''{name}'''\",\n",
    ")\n",
    "chain = langchain.LLMChain(prompt=prompt, llm=local_llm)\n",
    "chain.run(\"What are competitors to Apache Kafka?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18410b92a6894dc8b1b768e78f879dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29483c6378bb45fab4b52adb42ebdeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_as_a_judge_rubric.txt:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fbf63ca7eb4a3ea75563492437f623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bmc2_llm_judge_example_1.txt:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382cd7a063524f8aba0d11184e687562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b618e6498f4e472ca94bfe5cab661dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dalle_prompt.txt:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3682079009450298a5251ccbbdde2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7dd365124a4d5b85a8e76dfdcaeea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bmc2_llm_judge_example_2.txt:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a505f1a24242269a268c825983d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)nthetic_text_to_sql_train.snappy.parquet:   0%|          | 0.00/32.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82ef14775b948a580cb48ba31a5f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ynthetic_text_to_sql_test.snappy.parquet:   0%|          | 0.00/1.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/arda/Documents/llama-finetune/data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from HuggingFace\n",
    "dataset_name = \"gretelai/synthetic_text_to_sql\"\n",
    "save_dir = \"./data/synthetic_text_to_sql/\"\n",
    "\n",
    "huggingface_hub.snapshot_download(repo_id=dataset_name, repo_type=\"dataset\", local_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  What is the total volume of timber sold by eac...   \n",
      "1  List all the unique equipment types and their ...   \n",
      "2  How many marine species are found in the South...   \n",
      "3  What is the total trade value and average pric...   \n",
      "4  Find the energy efficiency upgrades with the h...   \n",
      "5  What is the total spending on humanitarian ass...   \n",
      "6  What is the average water temperature for each...   \n",
      "7  Delete a program's outcome data with given SQL...   \n",
      "8  Find the total fare collected from passengers ...   \n",
      "9  What is the average property size in inclusive...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT salesperson_id, name, SUM(volume) ...  \n",
      "1  SQL: SELECT equipment_type, SUM(maintenance_fr...  \n",
      "2  SQL: SELECT COUNT(*) FROM marine_species WHERE...  \n",
      "3  SQL: SELECT trader_id, stock, SUM(price * quan...  \n",
      "4  SQL: SELECT type, cost FROM (SELECT type, cost...  \n",
      "5  SQL: SELECT SUM(spending) FROM defense.eu_huma...  \n",
      "6  SQL: SELECT SpeciesName, AVG(WaterTemp) as Avg...  \n",
      "7  SQL: DELETE FROM Program_Outcomes WHERE progra...  \n",
      "8  SQL: SELECT SUM(fare) FROM bus_routes WHERE ro...  \n",
      "9  SQL: SELECT AVG(Property_Size) FROM Inclusive_...  \n",
      "                                              prompt  \\\n",
      "0  What is the average explainability score of cr...   \n",
      "1  Delete all records of rural infrastructure pro...   \n",
      "2  How many accidents have been recorded for Spac...   \n",
      "3  What is the maximum quantity of seafood sold i...   \n",
      "4  What is the total budget for movies released b...   \n",
      "5  Add a new attorney named 'Oliver Martinez' wit...   \n",
      "6  Identify the top 2 plants with the highest CO2...   \n",
      "7  What is the total cost of all climate communic...   \n",
      "8  List all marine species with their conservatio...   \n",
      "9  What is the average number of publications per...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT AVG(explainability_score) FROM cre...  \n",
      "1  SQL: DELETE FROM rural_infrastructure WHERE co...  \n",
      "2  SQL: SELECT launch_provider, COUNT(*) FROM Acc...  \n",
      "3  SQL: SELECT MAX(quantity) FROM sales;Explanati...  \n",
      "4  SQL: SELECT SUM(budget) FROM Movies_Release_Ye...  \n",
      "5  SQL: INSERT INTO attorneys (attorney_name, att...  \n",
      "6  SQL: SELECT plant_name, SUM(co2_emission_per_t...  \n",
      "7  SQL: SELECT SUM(total_cost) FROM climate_commu...  \n",
      "8  SQL: SELECT name, conservation_status FROM mar...  \n",
      "9  SQL: SELECT organization, AVG(publications) as...  \n",
      "                                                 prompt  \\\n",
      "3900  What is the total number of tickets sold for a...   \n",
      "3901  What is the total revenue for the soccer team ...   \n",
      "3902  Identify the number of security incidents that...   \n",
      "3903  Identify the top 5 threat intelligence sources...   \n",
      "3904  What are the collective bargaining agreements ...   \n",
      "3905  How many vessels arrived in Brazil in July 202...   \n",
      "3906  What was the maximum cargo weight for vessels ...   \n",
      "3907  Find the top 3 regions with the highest water ...   \n",
      "3908  List all water sources located in California, ...   \n",
      "3909  What is the average bias score for each attrib...   \n",
      "\n",
      "                                             completion  \n",
      "3900  SQL: SELECT SUM(quantity) FROM tickets INNER J...  \n",
      "3901  SQL: SELECT SUM(tickets.quantity * games.price...  \n",
      "3902  SQL: SELECT COUNT(*) FROM incidents WHERE inci...  \n",
      "3903  SQL: SELECT source, SUM(incident_count) as tot...  \n",
      "3904  SQL: SELECT UnionName, ExpirationDate FROM CBA...  \n",
      "3905  SQL: SELECT COUNT(*) FROM vessel_performance W...  \n",
      "3906  SQL: SELECT MAX(cargo_weight) FROM vessels WHE...  \n",
      "3907  SQL: SELECT region, SUM(efforts) AS total_effo...  \n",
      "3908  SQL: SELECT * FROM water_sources WHERE locatio...  \n",
      "3909  SQL: SELECT algorithm, attribute, AVG(bias_sco...  \n",
      "                                              prompt  \\\n",
      "0  What is the total volume of timber sold by eac...   \n",
      "1  List all the unique equipment types and their ...   \n",
      "2  How many marine species are found in the South...   \n",
      "3  What is the total trade value and average pric...   \n",
      "4  Find the energy efficiency upgrades with the h...   \n",
      "5  What is the total spending on humanitarian ass...   \n",
      "6  What is the average water temperature for each...   \n",
      "7  Delete a program's outcome data with given SQL...   \n",
      "8  Find the total fare collected from passengers ...   \n",
      "9  What is the average property size in inclusive...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT salesperson_id, name, SUM(volume) ...  \n",
      "1  SQL: SELECT equipment_type, SUM(maintenance_fr...  \n",
      "2  SQL: SELECT COUNT(*) FROM marine_species WHERE...  \n",
      "3  SQL: SELECT trader_id, stock, SUM(price * quan...  \n",
      "4  SQL: SELECT type, cost FROM (SELECT type, cost...  \n",
      "5  SQL: SELECT SUM(spending) FROM defense.eu_huma...  \n",
      "6  SQL: SELECT SpeciesName, AVG(WaterTemp) as Avg...  \n",
      "7  SQL: DELETE FROM Program_Outcomes WHERE progra...  \n",
      "8  SQL: SELECT SUM(fare) FROM bus_routes WHERE ro...  \n",
      "9  SQL: SELECT AVG(Property_Size) FROM Inclusive_...  \n",
      "                                              prompt  \\\n",
      "0  What is the average explainability score of cr...   \n",
      "1  Delete all records of rural infrastructure pro...   \n",
      "2  How many accidents have been recorded for Spac...   \n",
      "3  What is the maximum quantity of seafood sold i...   \n",
      "4  What is the total budget for movies released b...   \n",
      "5  Add a new attorney named 'Oliver Martinez' wit...   \n",
      "6  Identify the top 2 plants with the highest CO2...   \n",
      "7  What is the total cost of all climate communic...   \n",
      "8  List all marine species with their conservatio...   \n",
      "9  What is the average number of publications per...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT AVG(explainability_score) FROM cre...  \n",
      "1  SQL: DELETE FROM rural_infrastructure WHERE co...  \n",
      "2  SQL: SELECT launch_provider, COUNT(*) FROM Acc...  \n",
      "3  SQL: SELECT MAX(quantity) FROM sales;Explanati...  \n",
      "4  SQL: SELECT SUM(budget) FROM Movies_Release_Ye...  \n",
      "5  SQL: INSERT INTO attorneys (attorney_name, att...  \n",
      "6  SQL: SELECT plant_name, SUM(co2_emission_per_t...  \n",
      "7  SQL: SELECT SUM(total_cost) FROM climate_commu...  \n",
      "8  SQL: SELECT name, conservation_status FROM mar...  \n",
      "9  SQL: SELECT organization, AVG(publications) as...  \n",
      "                                                 prompt  \\\n",
      "3900  What is the total number of tickets sold for a...   \n",
      "3901  What is the total revenue for the soccer team ...   \n",
      "3902  Identify the number of security incidents that...   \n",
      "3903  Identify the top 5 threat intelligence sources...   \n",
      "3904  What are the collective bargaining agreements ...   \n",
      "3905  How many vessels arrived in Brazil in July 202...   \n",
      "3906  What was the maximum cargo weight for vessels ...   \n",
      "3907  Find the top 3 regions with the highest water ...   \n",
      "3908  List all water sources located in California, ...   \n",
      "3909  What is the average bias score for each attrib...   \n",
      "\n",
      "                                             completion  \n",
      "3900  SQL: SELECT SUM(quantity) FROM tickets INNER J...  \n",
      "3901  SQL: SELECT SUM(tickets.quantity * games.price...  \n",
      "3902  SQL: SELECT COUNT(*) FROM incidents WHERE inci...  \n",
      "3903  SQL: SELECT source, SUM(incident_count) as tot...  \n",
      "3904  SQL: SELECT UnionName, ExpirationDate FROM CBA...  \n",
      "3905  SQL: SELECT COUNT(*) FROM vessel_performance W...  \n",
      "3906  SQL: SELECT MAX(cargo_weight) FROM vessels WHE...  \n",
      "3907  SQL: SELECT region, SUM(efforts) AS total_effo...  \n",
      "3908  SQL: SELECT * FROM water_sources WHERE locatio...  \n",
      "3909  SQL: SELECT algorithm, attribute, AVG(bias_sco...  \n"
     ]
    }
   ],
   "source": [
    "from data.prepare import prepare_train,prepare_test_valid\n",
    "\n",
    "prepare_train()\n",
    "prepare_test_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLX Lib Model Loading-Training-Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdb8c4b9e0943feb459c2ce68e898d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Model load from mlx lib\n",
    "MODEL_ID=\"mlx-community/Llama-3.2-3B-Instruct-4bit\" # This is quantized model that not require quantization process.\n",
    "model, tokenizer = mlx_lm.load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are competitors to Apache Kafka?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "==========\n",
      "There are several competitors to Apache Kafka in the market. Here are some of the notable ones:\n",
      "\n",
      "1. **Amazon Kinesis**: Amazon Kinesis is a fast, reliable, and scalable platform for real-time data processing and analysis. It's designed to handle large volumes of data from various sources and provides features like data streaming, processing, and analytics.\n",
      "\n",
      "2. **Google Cloud Pub/Sub**: Google Cloud Pub/Sub is a messaging system that allows publishers to send messages to subscribers. It's designed to handle large volumes of data and provides features like message queuing, routing, and transformation.\n",
      "\n",
      "3. **Microsoft Azure Event Grid**: Azure Event Grid is a cloud-based event bus that allows publishers to send events to subscribers. It's designed to handle large volumes of data and provides features like event routing, transformation, and filtering.\n",
      "\n",
      "4. **RabbitMQ**: RabbitMQ is a message broker that allows publishers to send messages to subscribers. It's designed to handle large volumes of data and provides features like message queuing, routing, and transformation.\n",
      "\n",
      "5. **Amazon SQS**: Amazon SQS (Simple Queue Service) is a message queue service that allows publishers to send messages to subscribers. It's designed to handle large volumes of data and provides features like message queuing, routing,\n",
      "==========\n",
      "Prompt: 42 tokens, 77.838 tokens-per-sec\n",
      "Generation: 256 tokens, 69.411 tokens-per-sec\n",
      "Peak memory: 1.874 GB\n"
     ]
    }
   ],
   "source": [
    "# Default Template\n",
    "user_content=\"What are competitors to Apache Kafka?\"\n",
    "\n",
    "if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    print(prompt)\n",
    "\n",
    "response = mlx_lm.generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 13 Feb 2025\n",
      "You are the support assistant for questions that are asked to you.\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are competitors to Apache Kafka?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "# Custom Template\n",
    "def custom_chat_template(messages):\n",
    "    \"\"\"Creates LLaMA 3.2 custom chat template.\"\"\"\n",
    "    chat_str = \"<|begin_of_text|>\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            chat_str += f\"<|start_header_id|>system<|end_header_id|>\\n\\n{msg['content']}\\n\\n<|eot_id|>\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            chat_str += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_str += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\\n\"\n",
    "    chat_str += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    return chat_str\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Cutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\nYou are the support assistant for questions that are asked to you.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are competitors to Apache Kafka?\"}\n",
    "]\n",
    "\n",
    "prompt = custom_chat_template(messages)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "Apache Kafka has several competitors in the market. Some of the notable ones are:\n",
      "\n",
      "1. Amazon Kinesis: Amazon Kinesis is a fast, reliable, and scalable stream processing service offered by Amazon Web Services (AWS). It's designed to handle large amounts of data in real-time and provides features like data processing, event-driven architecture, and real-time analytics.\n",
      "\n",
      "2. RabbitMQ: RabbitMQ is a message broker that supports multiple messaging patterns, including pub/sub, request/reply, and message queues. It's known for its high performance, reliability, and scalability.\n",
      "\n",
      "3. Apache Storm: Apache Storm is a distributed real-time computation system that can handle large amounts of data in real-time. It's designed to handle high-throughput and provides features like fault-tolerant, scalable, and real-time processing.\n",
      "\n",
      "4. Google Cloud Pub/Sub: Google Cloud Pub/Sub is a messaging service offered by Google Cloud Platform. It's designed to handle large amounts of data in real-time and provides features like high-throughput, low-latency, and scalable.\n",
      "\n",
      "5. Microsoft Azure Event Grid: Microsoft Azure Event Grid is a service offered by Microsoft Azure that allows you to receive notifications when data changes in your applications. It's designed to handle large amounts of data in real-time and\n",
      "==========\n",
      "Prompt: 56 tokens, 389.941 tokens-per-sec\n",
      "Generation: 256 tokens, 68.652 tokens-per-sec\n",
      "Peak memory: 1.875 GB\n"
     ]
    }
   ],
   "source": [
    "response = mlx_lm.generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mlx_lm.convert in mlx_lm:\n",
      "\n",
      "NAME\n",
      "    mlx_lm.convert - # Copyright © 2023-2024 Apple Inc.\n",
      "\n",
      "FUNCTIONS\n",
      "    configure_parser() -> argparse.ArgumentParser\n",
      "        Configures and returns the argument parser for the script.\n",
      "\n",
      "        Returns:\n",
      "            argparse.ArgumentParser: Configured argument parser.\n",
      "\n",
      "    main()\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/mlx_lm/convert.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlx_lm.convert as convert\n",
    "help(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2986f32ad654e5593ce32b110776190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 8.500 bits per weight.\n"
     ]
    }
   ],
   "source": [
    "# Quantizing huggingface model with mlx.convert api\n",
    "# Our model is already quantized beacuse of this we are going to quantize new huggingface model.\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    hf_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    q_bits=8,\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"convert.py\",\n",
    "    \"--hf-path\", str(args.hf_path),\n",
    "    \"--q-bits\", str(args.q_bits),\n",
    "    \"--quantize\",\n",
    "]\n",
    "\n",
    "convert.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune - QLORA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package mlx_lm:\n",
      "\n",
      "NAME\n",
      "    mlx_lm - # Copyright © 2023-2024 Apple Inc.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _version\n",
      "    cache_prompt\n",
      "    chat\n",
      "    convert\n",
      "    evaluate\n",
      "    fuse\n",
      "    generate\n",
      "    gguf\n",
      "    lora\n",
      "    manage\n",
      "    merge\n",
      "    models (package)\n",
      "    sample_utils\n",
      "    server\n",
      "    tokenizer_utils\n",
      "    tuner (package)\n",
      "    utils\n",
      "\n",
      "VERSION\n",
      "    0.21.2\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/mlx_lm/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mlx_lm)\n",
    "import argparse\n",
    "from mlx_lm import lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a68aac5bbd4c898e834f802162916d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.071% (2.294M/3212.750M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 2.246, Val took 21.200s\n",
      "Iter 10: Train loss 2.166, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 210.051, Trained Tokens 4394, Peak mem 10.933 GB\n",
      "Iter 20: Train loss 1.593, Learning Rate 1.000e-05, It/sec 0.552, Tokens/sec 220.859, Trained Tokens 8392, Peak mem 10.933 GB\n",
      "Iter 30: Train loss 1.168, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 224.946, Trained Tokens 12414, Peak mem 10.933 GB\n",
      "Iter 40: Train loss 0.922, Learning Rate 1.000e-05, It/sec 0.466, Tokens/sec 226.470, Trained Tokens 17272, Peak mem 11.278 GB\n",
      "Iter 50: Train loss 0.760, Learning Rate 1.000e-05, It/sec 0.538, Tokens/sec 235.703, Trained Tokens 21652, Peak mem 11.278 GB\n",
      "Iter 60: Train loss 0.688, Learning Rate 1.000e-05, It/sec 0.471, Tokens/sec 233.109, Trained Tokens 26597, Peak mem 11.278 GB\n",
      "Iter 70: Train loss 0.764, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 250.598, Trained Tokens 30889, Peak mem 11.278 GB\n",
      "Iter 80: Train loss 0.662, Learning Rate 1.000e-05, It/sec 0.498, Tokens/sec 228.089, Trained Tokens 35471, Peak mem 11.278 GB\n",
      "Iter 90: Train loss 0.747, Learning Rate 1.000e-05, It/sec 0.539, Tokens/sec 249.435, Trained Tokens 40103, Peak mem 11.278 GB\n",
      "Iter 100: Train loss 0.705, Learning Rate 1.000e-05, It/sec 0.618, Tokens/sec 252.376, Trained Tokens 44185, Peak mem 11.278 GB\n",
      "Iter 100: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.574, Tokens/sec 245.039, Trained Tokens 48455, Peak mem 11.278 GB\n",
      "Iter 120: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.497, Tokens/sec 251.274, Trained Tokens 53513, Peak mem 11.278 GB\n",
      "Iter 130: Train loss 0.714, Learning Rate 1.000e-05, It/sec 0.603, Tokens/sec 254.106, Trained Tokens 57729, Peak mem 11.278 GB\n",
      "Iter 140: Train loss 0.664, Learning Rate 1.000e-05, It/sec 0.541, Tokens/sec 246.672, Trained Tokens 62291, Peak mem 11.278 GB\n",
      "Iter 150: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.651, Tokens/sec 243.019, Trained Tokens 66025, Peak mem 11.278 GB\n",
      "Iter 160: Train loss 0.677, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 241.491, Trained Tokens 70615, Peak mem 11.278 GB\n",
      "Iter 170: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.600, Tokens/sec 245.477, Trained Tokens 74705, Peak mem 11.278 GB\n",
      "Iter 180: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.620, Tokens/sec 247.760, Trained Tokens 78701, Peak mem 11.278 GB\n",
      "Iter 190: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 245.494, Trained Tokens 83206, Peak mem 12.106 GB\n",
      "Iter 200: Val loss 0.656, Val took 23.333s\n",
      "Iter 200: Train loss 0.582, Learning Rate 1.000e-05, It/sec 4.609, Tokens/sec 2289.965, Trained Tokens 88174, Peak mem 12.106 GB\n",
      "Iter 200: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.588, Learning Rate 1.000e-05, It/sec 0.456, Tokens/sec 227.196, Trained Tokens 93160, Peak mem 12.106 GB\n",
      "Iter 220: Train loss 0.616, Learning Rate 1.000e-05, It/sec 0.445, Tokens/sec 224.953, Trained Tokens 98210, Peak mem 12.106 GB\n",
      "Iter 230: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.492, Tokens/sec 226.822, Trained Tokens 102820, Peak mem 12.106 GB\n",
      "Iter 240: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.510, Tokens/sec 241.390, Trained Tokens 107550, Peak mem 12.106 GB\n",
      "Iter 250: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.577, Tokens/sec 250.235, Trained Tokens 111888, Peak mem 12.106 GB\n",
      "Iter 260: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 244.398, Trained Tokens 115982, Peak mem 12.106 GB\n",
      "Iter 270: Train loss 0.617, Learning Rate 1.000e-05, It/sec 0.485, Tokens/sec 242.927, Trained Tokens 120990, Peak mem 12.106 GB\n",
      "Iter 280: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 250.170, Trained Tokens 125132, Peak mem 12.106 GB\n",
      "Iter 290: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.476, Tokens/sec 247.535, Trained Tokens 130330, Peak mem 12.106 GB\n",
      "Iter 300: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.654, Tokens/sec 251.402, Trained Tokens 134172, Peak mem 12.106 GB\n",
      "Iter 300: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.637, Tokens/sec 246.751, Trained Tokens 138046, Peak mem 12.106 GB\n",
      "Iter 320: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.657, Tokens/sec 248.500, Trained Tokens 141830, Peak mem 12.106 GB\n",
      "Iter 330: Train loss 0.645, Learning Rate 1.000e-05, It/sec 0.662, Tokens/sec 250.279, Trained Tokens 145608, Peak mem 12.106 GB\n",
      "Iter 340: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 246.601, Trained Tokens 150110, Peak mem 12.106 GB\n",
      "Iter 350: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.615, Tokens/sec 245.428, Trained Tokens 154098, Peak mem 12.106 GB\n",
      "Iter 360: Train loss 0.658, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 248.240, Trained Tokens 158090, Peak mem 12.106 GB\n",
      "Iter 370: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 241.383, Trained Tokens 162522, Peak mem 12.106 GB\n",
      "Iter 380: Train loss 0.619, Learning Rate 1.000e-05, It/sec 0.546, Tokens/sec 249.920, Trained Tokens 167098, Peak mem 12.106 GB\n",
      "Iter 390: Train loss 0.651, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 241.141, Trained Tokens 171502, Peak mem 12.106 GB\n",
      "Iter 400: Val loss 0.618, Val took 22.647s\n",
      "Iter 400: Train loss 0.581, Learning Rate 1.000e-05, It/sec 5.096, Tokens/sec 2473.407, Trained Tokens 176356, Peak mem 12.106 GB\n",
      "Iter 400: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 247.711, Trained Tokens 180372, Peak mem 12.106 GB\n",
      "Iter 420: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.527, Tokens/sec 245.219, Trained Tokens 185022, Peak mem 12.106 GB\n",
      "Iter 430: Train loss 0.630, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 244.814, Trained Tokens 189338, Peak mem 12.106 GB\n",
      "Iter 440: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 252.601, Trained Tokens 193520, Peak mem 12.106 GB\n",
      "Iter 450: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 237.136, Trained Tokens 197492, Peak mem 12.106 GB\n",
      "Iter 460: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.511, Tokens/sec 238.764, Trained Tokens 202167, Peak mem 12.106 GB\n",
      "Iter 470: Train loss 0.648, Learning Rate 1.000e-05, It/sec 0.477, Tokens/sec 194.447, Trained Tokens 206242, Peak mem 12.106 GB\n",
      "Iter 480: Train loss 0.601, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 254.598, Trained Tokens 210604, Peak mem 12.106 GB\n",
      "Iter 490: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.605, Tokens/sec 243.318, Trained Tokens 214624, Peak mem 12.106 GB\n",
      "Iter 500: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.625, Tokens/sec 248.248, Trained Tokens 218596, Peak mem 12.106 GB\n",
      "Iter 500: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 239.191, Trained Tokens 222840, Peak mem 12.106 GB\n",
      "Iter 520: Train loss 0.618, Learning Rate 1.000e-05, It/sec 0.547, Tokens/sec 232.257, Trained Tokens 227084, Peak mem 12.106 GB\n",
      "Iter 530: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 207.306, Trained Tokens 231422, Peak mem 12.106 GB\n",
      "Iter 540: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 238.023, Trained Tokens 235640, Peak mem 12.106 GB\n",
      "Iter 550: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 230.716, Trained Tokens 240140, Peak mem 12.106 GB\n",
      "Iter 560: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.488, Tokens/sec 226.623, Trained Tokens 244782, Peak mem 12.106 GB\n",
      "Iter 570: Train loss 0.607, Learning Rate 1.000e-05, It/sec 0.596, Tokens/sec 244.295, Trained Tokens 248884, Peak mem 12.106 GB\n",
      "Iter 580: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.611, Tokens/sec 233.846, Trained Tokens 252714, Peak mem 12.106 GB\n",
      "Iter 590: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.554, Tokens/sec 238.493, Trained Tokens 257022, Peak mem 12.106 GB\n",
      "Iter 600: Val loss 0.646, Val took 20.027s\n",
      "Iter 600: Train loss 0.607, Learning Rate 1.000e-05, It/sec 4.685, Tokens/sec 2164.955, Trained Tokens 261643, Peak mem 12.106 GB\n",
      "Iter 600: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.533, Tokens/sec 238.976, Trained Tokens 266129, Peak mem 12.106 GB\n",
      "Iter 620: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.534, Tokens/sec 243.171, Trained Tokens 270679, Peak mem 12.106 GB\n",
      "Iter 630: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.522, Tokens/sec 254.309, Trained Tokens 275553, Peak mem 12.106 GB\n",
      "Iter 640: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.663, Tokens/sec 241.131, Trained Tokens 279189, Peak mem 12.106 GB\n",
      "Iter 650: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.506, Tokens/sec 237.818, Trained Tokens 283889, Peak mem 12.106 GB\n",
      "Iter 660: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.857, Trained Tokens 287941, Peak mem 12.106 GB\n",
      "Iter 670: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.525, Tokens/sec 250.159, Trained Tokens 292709, Peak mem 12.106 GB\n",
      "Iter 680: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.451, Tokens/sec 250.664, Trained Tokens 298267, Peak mem 12.106 GB\n",
      "Iter 690: Train loss 0.598, Learning Rate 1.000e-05, It/sec 0.623, Tokens/sec 248.441, Trained Tokens 302257, Peak mem 12.106 GB\n",
      "Iter 700: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.443, Tokens/sec 239.808, Trained Tokens 307671, Peak mem 12.106 GB\n",
      "Iter 700: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 238.065, Trained Tokens 312193, Peak mem 12.106 GB\n",
      "Iter 720: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.599, Tokens/sec 253.881, Trained Tokens 316433, Peak mem 12.106 GB\n",
      "Iter 730: Train loss 0.627, Learning Rate 1.000e-05, It/sec 0.671, Tokens/sec 248.509, Trained Tokens 320139, Peak mem 12.106 GB\n",
      "Iter 740: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.614, Tokens/sec 249.034, Trained Tokens 324195, Peak mem 12.106 GB\n",
      "Iter 750: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.591, Tokens/sec 242.646, Trained Tokens 328303, Peak mem 12.106 GB\n",
      "Iter 760: Train loss 0.640, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 245.124, Trained Tokens 332967, Peak mem 12.106 GB\n",
      "Iter 770: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.644, Tokens/sec 245.625, Trained Tokens 336779, Peak mem 12.106 GB\n",
      "Iter 780: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.581, Tokens/sec 238.123, Trained Tokens 340877, Peak mem 12.106 GB\n",
      "Iter 790: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 243.045, Trained Tokens 344817, Peak mem 12.106 GB\n",
      "Iter 800: Val loss 0.658, Val took 21.425s\n",
      "Iter 800: Train loss 0.621, Learning Rate 1.000e-05, It/sec 4.594, Tokens/sec 2302.357, Trained Tokens 349829, Peak mem 12.106 GB\n",
      "Iter 800: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.499, Tokens/sec 253.345, Trained Tokens 354909, Peak mem 12.106 GB\n",
      "Iter 820: Train loss 0.625, Learning Rate 1.000e-05, It/sec 0.691, Tokens/sec 248.738, Trained Tokens 358509, Peak mem 12.106 GB\n",
      "Iter 830: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.587, Tokens/sec 242.270, Trained Tokens 362635, Peak mem 12.106 GB\n",
      "Iter 840: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 245.970, Trained Tokens 366704, Peak mem 12.106 GB\n",
      "Iter 850: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.627, Tokens/sec 248.142, Trained Tokens 370660, Peak mem 12.106 GB\n",
      "Iter 860: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 250.849, Trained Tokens 375546, Peak mem 12.106 GB\n",
      "Iter 870: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.583, Tokens/sec 242.341, Trained Tokens 379706, Peak mem 12.106 GB\n",
      "Iter 880: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.565, Tokens/sec 245.302, Trained Tokens 384050, Peak mem 12.106 GB\n",
      "Iter 890: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 249.611, Trained Tokens 388518, Peak mem 12.106 GB\n",
      "Iter 900: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 233.360, Trained Tokens 392796, Peak mem 12.106 GB\n",
      "Iter 900: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.575, Tokens/sec 235.257, Trained Tokens 396888, Peak mem 12.106 GB\n",
      "Iter 920: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.461, Tokens/sec 250.336, Trained Tokens 402324, Peak mem 12.106 GB\n",
      "Iter 930: Train loss 0.594, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.058, Trained Tokens 406358, Peak mem 12.106 GB\n",
      "Iter 940: Train loss 0.602, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 248.398, Trained Tokens 410742, Peak mem 12.106 GB\n",
      "Iter 950: Train loss 0.582, Learning Rate 1.000e-05, It/sec 0.553, Tokens/sec 253.048, Trained Tokens 415322, Peak mem 12.106 GB\n",
      "Iter 960: Train loss 0.667, Learning Rate 1.000e-05, It/sec 0.563, Tokens/sec 238.937, Trained Tokens 419566, Peak mem 12.106 GB\n",
      "Iter 970: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.524, Tokens/sec 245.174, Trained Tokens 424248, Peak mem 12.106 GB\n",
      "Iter 980: Train loss 0.635, Learning Rate 1.000e-05, It/sec 0.455, Tokens/sec 193.946, Trained Tokens 428514, Peak mem 12.106 GB\n",
      "Iter 990: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.601, Tokens/sec 253.396, Trained Tokens 432732, Peak mem 12.106 GB\n",
      "Iter 1000: Val loss 0.632, Val took 22.836s\n",
      "Iter 1000: Train loss 0.590, Learning Rate 1.000e-05, It/sec 3.968, Tokens/sec 1815.926, Trained Tokens 437308, Peak mem 12.106 GB\n",
      "Iter 1000: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0001000_adapters.safetensors.\n",
      "Saved final weights to adapters_llama_3b/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "# QLORA\n",
    "# Fine-Tune LLM Model- Since the model was quantized, it was applied to the lora without quantizing the model.\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    model=MODEL_ID,\n",
    "    data=\"./data/\",\n",
    "    train=True,\n",
    "    test=False,\n",
    "    batch_size=2,\n",
    "    num_layers=16,\n",
    "    iters=1000,\n",
    "    fine_tune_type=\"lora\",\n",
    "    resume_adapter_file=None,\n",
    "    adapter_path = \"adapters_llama_3b\",\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"lora.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--data\", str(args.data),\n",
    "    \"--train\",\n",
    "    \"--batch-size\", str(args.batch_size),\n",
    "    \"--num-layers\", str(args.num_layers),\n",
    "    \"--iters\", str(args.iters),\n",
    "    \"--fine-tune-type\", str(args.fine_tune_type),\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "    \"--seed\", str(args.seed)\n",
    "]\n",
    "\n",
    "lora.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL_ID=\"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "\n",
    "Loading datasets\n",
    "Training\n",
    "Trainable parameters: 0.071% (2.294M/3212.750M)\n",
    "Starting training..., iters: 1000\n",
    "Iter 1: Val loss 2.246, Val took 21.200s\n",
    "Iter 10: Train loss 2.166, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 210.051, Trained Tokens 4394, Peak mem 10.933 GB\n",
    "Iter 20: Train loss 1.593, Learning Rate 1.000e-05, It/sec 0.552, Tokens/sec 220.859, Trained Tokens 8392, Peak mem 10.933 GB\n",
    "Iter 30: Train loss 1.168, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 224.946, Trained Tokens 12414, Peak mem 10.933 GB\n",
    "Iter 40: Train loss 0.922, Learning Rate 1.000e-05, It/sec 0.466, Tokens/sec 226.470, Trained Tokens 17272, Peak mem 11.278 GB\n",
    "Iter 50: Train loss 0.760, Learning Rate 1.000e-05, It/sec 0.538, Tokens/sec 235.703, Trained Tokens 21652, Peak mem 11.278 GB\n",
    "Iter 60: Train loss 0.688, Learning Rate 1.000e-05, It/sec 0.471, Tokens/sec 233.109, Trained Tokens 26597, Peak mem 11.278 GB\n",
    "Iter 70: Train loss 0.764, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 250.598, Trained Tokens 30889, Peak mem 11.278 GB\n",
    "Iter 80: Train loss 0.662, Learning Rate 1.000e-05, It/sec 0.498, Tokens/sec 228.089, Trained Tokens 35471, Peak mem 11.278 GB\n",
    "Iter 90: Train loss 0.747, Learning Rate 1.000e-05, It/sec 0.539, Tokens/sec 249.435, Trained Tokens 40103, Peak mem 11.278 GB\n",
    "Iter 100: Train loss 0.705, Learning Rate 1.000e-05, It/sec 0.618, Tokens/sec 252.376, Trained Tokens 44185, Peak mem 11.278 GB\n",
    "Iter 100: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000100_adapters.safetensors.\n",
    "Iter 110: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.574, Tokens/sec 245.039, Trained Tokens 48455, Peak mem 11.278 GB\n",
    "Iter 120: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.497, Tokens/sec 251.274, Trained Tokens 53513, Peak mem 11.278 GB\n",
    "Iter 130: Train loss 0.714, Learning Rate 1.000e-05, It/sec 0.603, Tokens/sec 254.106, Trained Tokens 57729, Peak mem 11.278 GB\n",
    "Iter 140: Train loss 0.664, Learning Rate 1.000e-05, It/sec 0.541, Tokens/sec 246.672, Trained Tokens 62291, Peak mem 11.278 GB\n",
    "Iter 150: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.651, Tokens/sec 243.019, Trained Tokens 66025, Peak mem 11.278 GB\n",
    "Iter 160: Train loss 0.677, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 241.491, Trained Tokens 70615, Peak mem 11.278 GB\n",
    "Iter 170: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.600, Tokens/sec 245.477, Trained Tokens 74705, Peak mem 11.278 GB\n",
    "Iter 180: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.620, Tokens/sec 247.760, Trained Tokens 78701, Peak mem 11.278 GB\n",
    "Iter 190: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 245.494, Trained Tokens 83206, Peak mem 12.106 GB\n",
    "Iter 200: Val loss 0.656, Val took 23.333s\n",
    "Iter 200: Train loss 0.582, Learning Rate 1.000e-05, It/sec 4.609, Tokens/sec 2289.965, Trained Tokens 88174, Peak mem 12.106 GB\n",
    "Iter 200: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000200_adapters.safetensors.\n",
    "Iter 210: Train loss 0.588, Learning Rate 1.000e-05, It/sec 0.456, Tokens/sec 227.196, Trained Tokens 93160, Peak mem 12.106 GB\n",
    "Iter 220: Train loss 0.616, Learning Rate 1.000e-05, It/sec 0.445, Tokens/sec 224.953, Trained Tokens 98210, Peak mem 12.106 GB\n",
    "Iter 230: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.492, Tokens/sec 226.822, Trained Tokens 102820, Peak mem 12.106 GB\n",
    "Iter 240: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.510, Tokens/sec 241.390, Trained Tokens 107550, Peak mem 12.106 GB\n",
    "Iter 250: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.577, Tokens/sec 250.235, Trained Tokens 111888, Peak mem 12.106 GB\n",
    "Iter 260: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 244.398, Trained Tokens 115982, Peak mem 12.106 GB\n",
    "Iter 270: Train loss 0.617, Learning Rate 1.000e-05, It/sec 0.485, Tokens/sec 242.927, Trained Tokens 120990, Peak mem 12.106 GB\n",
    "Iter 280: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 250.170, Trained Tokens 125132, Peak mem 12.106 GB\n",
    "Iter 290: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.476, Tokens/sec 247.535, Trained Tokens 130330, Peak mem 12.106 GB\n",
    "Iter 300: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.654, Tokens/sec 251.402, Trained Tokens 134172, Peak mem 12.106 GB\n",
    "Iter 300: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000300_adapters.safetensors.\n",
    "Iter 310: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.637, Tokens/sec 246.751, Trained Tokens 138046, Peak mem 12.106 GB\n",
    "Iter 320: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.657, Tokens/sec 248.500, Trained Tokens 141830, Peak mem 12.106 GB\n",
    "Iter 330: Train loss 0.645, Learning Rate 1.000e-05, It/sec 0.662, Tokens/sec 250.279, Trained Tokens 145608, Peak mem 12.106 GB\n",
    "Iter 340: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 246.601, Trained Tokens 150110, Peak mem 12.106 GB\n",
    "Iter 350: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.615, Tokens/sec 245.428, Trained Tokens 154098, Peak mem 12.106 GB\n",
    "Iter 360: Train loss 0.658, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 248.240, Trained Tokens 158090, Peak mem 12.106 GB\n",
    "Iter 370: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 241.383, Trained Tokens 162522, Peak mem 12.106 GB\n",
    "Iter 380: Train loss 0.619, Learning Rate 1.000e-05, It/sec 0.546, Tokens/sec 249.920, Trained Tokens 167098, Peak mem 12.106 GB\n",
    "Iter 390: Train loss 0.651, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 241.141, Trained Tokens 171502, Peak mem 12.106 GB\n",
    "Iter 400: Val loss 0.618, Val took 22.647s\n",
    "Iter 400: Train loss 0.581, Learning Rate 1.000e-05, It/sec 5.096, Tokens/sec 2473.407, Trained Tokens 176356, Peak mem 12.106 GB\n",
    "Iter 400: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000400_adapters.safetensors.\n",
    "Iter 410: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 247.711, Trained Tokens 180372, Peak mem 12.106 GB\n",
    "Iter 420: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.527, Tokens/sec 245.219, Trained Tokens 185022, Peak mem 12.106 GB\n",
    "Iter 430: Train loss 0.630, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 244.814, Trained Tokens 189338, Peak mem 12.106 GB\n",
    "Iter 440: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 252.601, Trained Tokens 193520, Peak mem 12.106 GB\n",
    "Iter 450: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 237.136, Trained Tokens 197492, Peak mem 12.106 GB\n",
    "Iter 460: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.511, Tokens/sec 238.764, Trained Tokens 202167, Peak mem 12.106 GB\n",
    "Iter 470: Train loss 0.648, Learning Rate 1.000e-05, It/sec 0.477, Tokens/sec 194.447, Trained Tokens 206242, Peak mem 12.106 GB\n",
    "Iter 480: Train loss 0.601, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 254.598, Trained Tokens 210604, Peak mem 12.106 GB\n",
    "Iter 490: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.605, Tokens/sec 243.318, Trained Tokens 214624, Peak mem 12.106 GB\n",
    "Iter 500: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.625, Tokens/sec 248.248, Trained Tokens 218596, Peak mem 12.106 GB\n",
    "Iter 500: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000500_adapters.safetensors.\n",
    "Iter 510: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 239.191, Trained Tokens 222840, Peak mem 12.106 GB\n",
    "Iter 520: Train loss 0.618, Learning Rate 1.000e-05, It/sec 0.547, Tokens/sec 232.257, Trained Tokens 227084, Peak mem 12.106 GB\n",
    "Iter 530: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 207.306, Trained Tokens 231422, Peak mem 12.106 GB\n",
    "Iter 540: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 238.023, Trained Tokens 235640, Peak mem 12.106 GB\n",
    "Iter 550: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 230.716, Trained Tokens 240140, Peak mem 12.106 GB\n",
    "Iter 560: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.488, Tokens/sec 226.623, Trained Tokens 244782, Peak mem 12.106 GB\n",
    "Iter 570: Train loss 0.607, Learning Rate 1.000e-05, It/sec 0.596, Tokens/sec 244.295, Trained Tokens 248884, Peak mem 12.106 GB\n",
    "Iter 580: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.611, Tokens/sec 233.846, Trained Tokens 252714, Peak mem 12.106 GB\n",
    "Iter 590: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.554, Tokens/sec 238.493, Trained Tokens 257022, Peak mem 12.106 GB\n",
    "Iter 600: Val loss 0.646, Val took 20.027s\n",
    "Iter 600: Train loss 0.607, Learning Rate 1.000e-05, It/sec 4.685, Tokens/sec 2164.955, Trained Tokens 261643, Peak mem 12.106 GB\n",
    "Iter 600: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000600_adapters.safetensors.\n",
    "Iter 610: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.533, Tokens/sec 238.976, Trained Tokens 266129, Peak mem 12.106 GB\n",
    "Iter 620: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.534, Tokens/sec 243.171, Trained Tokens 270679, Peak mem 12.106 GB\n",
    "Iter 630: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.522, Tokens/sec 254.309, Trained Tokens 275553, Peak mem 12.106 GB\n",
    "Iter 640: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.663, Tokens/sec 241.131, Trained Tokens 279189, Peak mem 12.106 GB\n",
    "Iter 650: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.506, Tokens/sec 237.818, Trained Tokens 283889, Peak mem 12.106 GB\n",
    "Iter 660: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.857, Trained Tokens 287941, Peak mem 12.106 GB\n",
    "Iter 670: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.525, Tokens/sec 250.159, Trained Tokens 292709, Peak mem 12.106 GB\n",
    "Iter 680: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.451, Tokens/sec 250.664, Trained Tokens 298267, Peak mem 12.106 GB\n",
    "Iter 690: Train loss 0.598, Learning Rate 1.000e-05, It/sec 0.623, Tokens/sec 248.441, Trained Tokens 302257, Peak mem 12.106 GB\n",
    "Iter 700: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.443, Tokens/sec 239.808, Trained Tokens 307671, Peak mem 12.106 GB\n",
    "Iter 700: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000700_adapters.safetensors.\n",
    "Iter 710: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 238.065, Trained Tokens 312193, Peak mem 12.106 GB\n",
    "Iter 720: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.599, Tokens/sec 253.881, Trained Tokens 316433, Peak mem 12.106 GB\n",
    "Iter 730: Train loss 0.627, Learning Rate 1.000e-05, It/sec 0.671, Tokens/sec 248.509, Trained Tokens 320139, Peak mem 12.106 GB\n",
    "Iter 740: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.614, Tokens/sec 249.034, Trained Tokens 324195, Peak mem 12.106 GB\n",
    "Iter 750: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.591, Tokens/sec 242.646, Trained Tokens 328303, Peak mem 12.106 GB\n",
    "Iter 760: Train loss 0.640, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 245.124, Trained Tokens 332967, Peak mem 12.106 GB\n",
    "Iter 770: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.644, Tokens/sec 245.625, Trained Tokens 336779, Peak mem 12.106 GB\n",
    "Iter 780: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.581, Tokens/sec 238.123, Trained Tokens 340877, Peak mem 12.106 GB\n",
    "Iter 790: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 243.045, Trained Tokens 344817, Peak mem 12.106 GB\n",
    "Iter 800: Val loss 0.658, Val took 21.425s\n",
    "Iter 800: Train loss 0.621, Learning Rate 1.000e-05, It/sec 4.594, Tokens/sec 2302.357, Trained Tokens 349829, Peak mem 12.106 GB\n",
    "Iter 800: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000800_adapters.safetensors.\n",
    "Iter 810: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.499, Tokens/sec 253.345, Trained Tokens 354909, Peak mem 12.106 GB\n",
    "Iter 820: Train loss 0.625, Learning Rate 1.000e-05, It/sec 0.691, Tokens/sec 248.738, Trained Tokens 358509, Peak mem 12.106 GB\n",
    "Iter 830: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.587, Tokens/sec 242.270, Trained Tokens 362635, Peak mem 12.106 GB\n",
    "Iter 840: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 245.970, Trained Tokens 366704, Peak mem 12.106 GB\n",
    "Iter 850: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.627, Tokens/sec 248.142, Trained Tokens 370660, Peak mem 12.106 GB\n",
    "Iter 860: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 250.849, Trained Tokens 375546, Peak mem 12.106 GB\n",
    "Iter 870: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.583, Tokens/sec 242.341, Trained Tokens 379706, Peak mem 12.106 GB\n",
    "Iter 880: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.565, Tokens/sec 245.302, Trained Tokens 384050, Peak mem 12.106 GB\n",
    "Iter 890: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 249.611, Trained Tokens 388518, Peak mem 12.106 GB\n",
    "Iter 900: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 233.360, Trained Tokens 392796, Peak mem 12.106 GB\n",
    "Iter 900: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000900_adapters.safetensors.\n",
    "Iter 910: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.575, Tokens/sec 235.257, Trained Tokens 396888, Peak mem 12.106 GB\n",
    "Iter 920: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.461, Tokens/sec 250.336, Trained Tokens 402324, Peak mem 12.106 GB\n",
    "Iter 930: Train loss 0.594, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.058, Trained Tokens 406358, Peak mem 12.106 GB\n",
    "Iter 940: Train loss 0.602, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 248.398, Trained Tokens 410742, Peak mem 12.106 GB\n",
    "Iter 950: Train loss 0.582, Learning Rate 1.000e-05, It/sec 0.553, Tokens/sec 253.048, Trained Tokens 415322, Peak mem 12.106 GB\n",
    "Iter 960: Train loss 0.667, Learning Rate 1.000e-05, It/sec 0.563, Tokens/sec 238.937, Trained Tokens 419566, Peak mem 12.106 GB\n",
    "Iter 970: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.524, Tokens/sec 245.174, Trained Tokens 424248, Peak mem 12.106 GB\n",
    "Iter 980: Train loss 0.635, Learning Rate 1.000e-05, It/sec 0.455, Tokens/sec 193.946, Trained Tokens 428514, Peak mem 12.106 GB\n",
    "Iter 990: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.601, Tokens/sec 253.396, Trained Tokens 432732, Peak mem 12.106 GB\n",
    "Iter 1000: Val loss 0.632, Val took 22.836s\n",
    "Iter 1000: Train loss 0.590, Learning Rate 1.000e-05, It/sec 3.968, Tokens/sec 1815.926, Trained Tokens 437308, Peak mem 12.106 GB\n",
    "Iter 1000: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0001000_adapters.safetensors.\n",
    "Saved final weights to adapters_llama_3b/adapters.safetensors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have two choice 1- Using the cell code , 2- Terminal code\n",
    "\"\"\"\n",
    "myenv/bin/python -m mlx_lm.lora \\\n",
    "  --model meta-llama/Llama-3.1-8B-Instruct \\\n",
    "  --data data \\\n",
    "  --train \\\n",
    "  --batch-size 1 \\\n",
    "  --num-layers 8 \\\n",
    "  --iters 600 \\\n",
    "  --fine-tune-type lora\n",
    "\"\"\"\n",
    "#Output:\n",
    "\"\"\"\n",
    "\n",
    "Loading pretrained model\n",
    "Fetching 11 files: 100%|████████████████████| 11/11 [00:00<00:00, 118300.88it/s]\n",
    "Loading datasets\n",
    "Training\n",
    "Trainable parameters: 0.042% (3.408M/8030.261M)\n",
    "Starting training..., iters: 600\n",
    "Iter 1: Val loss 2.041, Val took 1722.526s\n",
    "Iter 10: Train loss 1.944, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.451, Trained Tokens 1916, Peak mem 18.338 GB\n",
    "Iter 20: Train loss 1.203, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.662, Trained Tokens 4077, Peak mem 18.338 GB\n",
    "Iter 30: Train loss 0.893, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.769, Trained Tokens 6232, Peak mem 18.545 GB\n",
    "Iter 40: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.675, Trained Tokens 8340, Peak mem 18.614 GB\n",
    "Iter 50: Train loss 0.560, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.572, Trained Tokens 10466, Peak mem 18.614 GB\n",
    "Iter 60: Train loss 0.690, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.537, Trained Tokens 12580, Peak mem 18.966 GB\n",
    "Iter 70: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.811, Trained Tokens 14698, Peak mem 18.966 GB\n",
    "Iter 80: Train loss 0.544, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.827, Trained Tokens 16931, Peak mem 19.108 GB\n",
    "Iter 90: Train loss 0.542, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.823, Trained Tokens 19161, Peak mem 19.469 GB\n",
    "Iter 100: Train loss 0.572, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.688, Trained Tokens 21316, Peak mem 19.469 GB\n",
    "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
    "Iter 110: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.323, Trained Tokens 23136, Peak mem 19.469 GB\n",
    "Iter 120: Train loss 0.569, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.672, Trained Tokens 25198, Peak mem 19.469 GB\n",
    "Iter 130: Train loss 0.558, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.794, Trained Tokens 27446, Peak mem 19.469 GB\n",
    "Iter 140: Train loss 0.593, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.597, Trained Tokens 29539, Peak mem 19.469 GB\n",
    "Iter 150: Train loss 0.514, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.728, Trained Tokens 31703, Peak mem 19.469 GB\n",
    "Iter 160: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.725, Trained Tokens 33645, Peak mem 19.469 GB\n",
    "Iter 170: Train loss 0.534, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.731, Trained Tokens 35593, Peak mem 19.469 GB\n",
    "Iter 180: Train loss 0.547, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.921, Trained Tokens 37819, Peak mem 19.469 GB\n",
    "Iter 190: Train loss 0.501, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.453, Trained Tokens 39666, Peak mem 19.469 GB\n",
    "Iter 200: Val loss 0.538, Val took 1592.634s\n",
    "Iter 200: Train loss 0.501, Learning Rate 1.000e-05, It/sec 0.116, Tokens/sec 28.023, Trained Tokens 42088, Peak mem 19.469 GB\n",
    "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
    "Iter 210: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.710, Trained Tokens 44153, Peak mem 19.469 GB\n",
    "Iter 220: Train loss 0.530, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.055, Trained Tokens 46397, Peak mem 19.469 GB\n",
    "Iter 230: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.109, Trained Tokens 48594, Peak mem 20.605 GB\n",
    "Iter 240: Train loss 0.531, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.703, Trained Tokens 50617, Peak mem 20.605 GB\n",
    "Iter 250: Train loss 0.469, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.777, Trained Tokens 52803, Peak mem 20.605 GB\n",
    "Iter 260: Train loss 0.442, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.347, Trained Tokens 55329, Peak mem 20.605 GB\n",
    "Iter 270: Train loss 0.490, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.150, Trained Tokens 57556, Peak mem 20.605 GB\n",
    "Iter 280: Train loss 0.483, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.266, Trained Tokens 60085, Peak mem 20.914 GB\n",
    "Iter 290: Train loss 0.527, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.849, Trained Tokens 62247, Peak mem 20.914 GB\n",
    "Iter 300: Train loss 0.492, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.589, Trained Tokens 64205, Peak mem 20.914 GB\n",
    "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
    "Iter 310: Train loss 0.513, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.902, Trained Tokens 66314, Peak mem 20.914 GB\n",
    "Iter 320: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.007, Trained Tokens 68574, Peak mem 20.914 GB\n",
    "Iter 330: Train loss 0.521, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.760, Trained Tokens 70660, Peak mem 20.914 GB\n",
    "Iter 340: Train loss 0.541, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.534, Trained Tokens 72612, Peak mem 20.914 GB\n",
    "Iter 350: Train loss 0.519, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.869, Trained Tokens 74696, Peak mem 20.914 GB\n",
    "Iter 360: Train loss 0.504, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.757, Trained Tokens 76714, Peak mem 20.914 GB\n",
    "Iter 370: Train loss 0.487, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.765, Trained Tokens 78802, Peak mem 20.914 GB\n",
    "Iter 380: Train loss 0.542, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.315, Trained Tokens 80586, Peak mem 20.914 GB\n",
    "Iter 390: Train loss 0.521, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.463, Trained Tokens 83198, Peak mem 20.914 GB\n",
    "Iter 400: Val loss 0.514, Val took 1662.526s\n",
    "Iter 400: Train loss 0.477, Learning Rate 1.000e-05, It/sec 0.115, Tokens/sec 26.611, Trained Tokens 85519, Peak mem 20.914 GB\n",
    "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
    "Iter 410: Train loss 0.474, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.279, Trained Tokens 87424, Peak mem 20.914 GB\n",
    "Iter 420: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.011, Trained Tokens 89647, Peak mem 20.914 GB\n",
    "Iter 430: Train loss 0.504, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.087, Trained Tokens 92035, Peak mem 20.914 GB\n",
    "Iter 440: Train loss 0.495, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.046, Trained Tokens 94466, Peak mem 20.914 GB\n",
    "Iter 450: Train loss 0.503, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.672, Trained Tokens 96499, Peak mem 20.914 GB\n",
    "Iter 460: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.886, Trained Tokens 98635, Peak mem 20.914 GB\n",
    "Iter 470: Train loss 0.504, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.064, Trained Tokens 100886, Peak mem 20.914 GB\n",
    "Iter 480: Train loss 0.468, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.955, Trained Tokens 103305, Peak mem 20.914 GB\n",
    "Iter 490: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.808, Trained Tokens 105436, Peak mem 20.914 GB\n",
    "Iter 500: Train loss 0.495, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.799, Trained Tokens 107603, Peak mem 20.914 GB\n",
    "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
    "Iter 510: Train loss 0.584, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.285, Trained Tokens 110072, Peak mem 20.914 GB\n",
    "Iter 520: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.832, Trained Tokens 112253, Peak mem 20.914 GB\n",
    "Iter 530: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.327, Trained Tokens 114805, Peak mem 20.914 GB\n",
    "Iter 540: Train loss 0.464, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.267, Trained Tokens 117348, Peak mem 20.914 GB\n",
    "Iter 550: Train loss 0.513, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.112, Trained Tokens 119569, Peak mem 20.914 GB\n",
    "Iter 560: Train loss 0.438, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.056, Trained Tokens 121756, Peak mem 20.914 GB\n",
    "Iter 570: Train loss 0.514, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.713, Trained Tokens 123820, Peak mem 20.914 GB\n",
    "Iter 580: Train loss 0.525, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.026, Trained Tokens 126135, Peak mem 20.914 GB\n",
    "Iter 590: Train loss 0.520, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.965, Trained Tokens 128242, Peak mem 20.914 GB\n",
    "Iter 600: Val loss 0.507, Val took 1597.140s\n",
    "Iter 600: Train loss 0.532, Learning Rate 1.000e-05, It/sec 0.089, Tokens/sec 16.756, Trained Tokens 130122, Peak mem 20.914 GB\n",
    "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
    "Saved final weights to adapters/adapters.safetensors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Test loss 0.620, Test ppl 1.859.\n"
     ]
    }
   ],
   "source": [
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"lora.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--data\", str(args.data),\n",
    "    \"--test\",\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "]\n",
    "\n",
    "lora.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the model decides between about 1,859 different words for each word. This is a good result for language models (Usually PPL < 10 is considered good, PPL ~ 2 is close to perfect).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained vs Fine Tuned LLM Output Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "I don't have any information about transactions or customers from the 'Africa' region. I'm a text-based AI assistant and do not have access to any specific data or databases. If you could provide more context or clarify what you are referring to, I'll do my best to help.\n",
      "==========\n",
      "Prompt: 61 tokens, 71.435 tokens-per-sec\n",
      "Generation: 60 tokens, 68.330 tokens-per-sec\n",
      "Peak memory: 1.875 GB\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "\n",
    "messages_base = [\n",
    "    {\"role\": \"system\", \"content\": \"Cutting Knowledge Date: December 2023\\nToday Date: 14 Feb 2025\\nYou are the support assistant for questions that are asked to you.\"},\n",
    "    {\"role\": \"user\", \"content\": \"List all transactions and customers from the 'Africa' region.\"}\n",
    "]\n",
    "\n",
    "prompt_base = custom_chat_template(messages_base)\n",
    "response = mlx_lm.generate(model, tokenizer, prompt=prompt_base, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "SQL: SELECT transactions.id, customers.name, transactions.amount, customers.country FROM transactions JOIN customers ON transactions.customer_id = customers.id WHERE customers.region = 'Africa';\n",
      "==========\n",
      "Prompt: 62 tokens, 160.189 tokens-per-sec\n",
      "Generation: 34 tokens, 51.437 tokens-per-sec\n",
      "Peak memory: 3.686 GB\n"
     ]
    }
   ],
   "source": [
    "# Fine Tuned Model\n",
    "\n",
    "import mlx_lm.generate as generate\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "messages_ft = [\n",
    "    {\"role\": \"system\", \"content\": \"Cutting Knowledge Date: December 2023\\nToday Date: 14 Feb 2025\\nYou are the support assistant for questions that are asked to you.\"},\n",
    "    {\"role\": \"user\", \"content\": \"List all transactions and customers from the 'Africa' region.\"}\n",
    "]\n",
    "\n",
    "prompt_ft = custom_chat_template(messages_ft)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    model=MODEL_ID,\n",
    "    adapter_path = \"./adapters_llama_3b/\",\n",
    "    prompt = prompt_ft,\n",
    "    max_tokens = 500,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"generate.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--ignore-chat-template\",\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "    \"--prompt\",str(args.prompt),\n",
    "    \"--max-tokens\",str(args.max_tokens),\n",
    "    \"--verbose\",str(args.verbose)\n",
    "]\n",
    "\n",
    "generate.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse Adapters to Model for Building New LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mlx_lm.fuse in mlx_lm:\n",
      "\n",
      "NAME\n",
      "    mlx_lm.fuse\n",
      "\n",
      "FUNCTIONS\n",
      "    main() -> None\n",
      "\n",
      "    parse_arguments() -> argparse.Namespace\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/mlx_lm/fuse.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import fuse\n",
    "\n",
    "help(fuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff71ff33041c4997a8d8212bb04c9881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-quantizing model\n",
      "Converting to GGUF format\n",
      "Converted GGUF model saved as: sql_agent/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    model=MODEL_ID,\n",
    "    adapter_path = \"./adapters_llama_3b/\",\n",
    "    prompt = prompt_ft,\n",
    "    max_tokens = 500,\n",
    "    verbose = True,\n",
    "    save_path = \"./sql_agent/\",\n",
    "    gguf_path = \"./sql_agent/gguf_model/ggml-model-sql-agent.gguf\"\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"fuse.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "    \"--de-quantize\", #Its because dequantize models not supported with gguf format.\n",
    "    \"--save-path\", str(args.save_path),\n",
    "    \"--export-gguf\",\n",
    "    #\"--gguf-path\", str(args.gguf_path)\n",
    "]\n",
    "\n",
    "fuse.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ollama api and keep it running.\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create models dir\n",
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model file \n",
    "!nano models/Modelfile FROM ../sql_agent/ggml-model-f16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 0% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 0% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 1% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 2% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 4% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 5% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 7% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 8% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 9% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 10% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 12% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 13% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 15% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 16% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 17% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 18% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 20% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 21% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 23% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 24% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 25% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 26% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 28% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 29% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 30% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 32% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 33% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 34% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 35% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 37% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 38% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 40% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 41% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 43% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 44% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 45% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 46% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 48% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 49% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 50% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 52% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 54% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 55% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 56% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 58% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 59% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 60% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 62% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 63% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 64% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 66% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 67% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 69% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 71% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 71% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 73% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 75% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 76% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 77% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 79% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 80% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 82% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 84% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 85% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 87% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 88% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 89% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 91% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 93% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 95% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 96% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 98% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 98% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "#Create Ollama model\n",
    "!ollama create sql-agent -f models/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED      \n",
      "sql-agent:latest    c6b495001637    6.4 GB    7 minutes ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama run sql-agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
