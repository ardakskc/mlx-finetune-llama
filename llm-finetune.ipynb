{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get necessary libs\n",
    "import os\n",
    "import mlx_lm\n",
    "import torch\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Model Loading and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Login HuggingFace with API Key\n",
    "import huggingface_hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "MODEL_ID=\"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
    "\n",
    "huggingface_hub.login(HUGGING_FACE_API_KEY)\n",
    "# Download Pre-trained LLM\n",
    "huggingface_hub.snapshot_download(repo_id=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Connectivity for can we use model offline:\n",
    "# Step 1: Turn Off Wifi\n",
    "# Step 2: Run Cell and Test LLM Outputs\n",
    "# Step 3: Turn On Wifi and go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline,AutoTokenizer,AutoModelForCausalLM,AutoModelForSeq2SeqLM,BitsAndBytesConfig\n",
    "\n",
    "#Load model with 4-bit mode. This cause low memory usage.\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_use_double_quant=True) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,device_map=\"auto\",quantization_config=quantization_config)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,max_length=128)\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get output with Langchain Prompt Template\n",
    "prompt = langchain.PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"Can you answer this question: '''{name}'''\",\n",
    ")\n",
    "chain = langchain.LLMChain(prompt=prompt, llm=local_llm)\n",
    "chain.run(\"What are competitors to Apache Kafka?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18410b92a6894dc8b1b768e78f879dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29483c6378bb45fab4b52adb42ebdeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_as_a_judge_rubric.txt:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fbf63ca7eb4a3ea75563492437f623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bmc2_llm_judge_example_1.txt:   0%|          | 0.00/1.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382cd7a063524f8aba0d11184e687562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/2.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b618e6498f4e472ca94bfe5cab661dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dalle_prompt.txt:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3682079009450298a5251ccbbdde2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7dd365124a4d5b85a8e76dfdcaeea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bmc2_llm_judge_example_2.txt:   0%|          | 0.00/2.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a505f1a24242269a268c825983d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)nthetic_text_to_sql_train.snappy.parquet:   0%|          | 0.00/32.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82ef14775b948a580cb48ba31a5f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ynthetic_text_to_sql_test.snappy.parquet:   0%|          | 0.00/1.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/arda/Documents/llama-finetune/data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from HuggingFace\n",
    "dataset_name = \"gretelai/synthetic_text_to_sql\"\n",
    "save_dir = \"./data/synthetic_text_to_sql/\"\n",
    "\n",
    "huggingface_hub.snapshot_download(repo_id=dataset_name, repo_type=\"dataset\", local_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  What is the total volume of timber sold by eac...   \n",
      "1  List all the unique equipment types and their ...   \n",
      "2  How many marine species are found in the South...   \n",
      "3  What is the total trade value and average pric...   \n",
      "4  Find the energy efficiency upgrades with the h...   \n",
      "5  What is the total spending on humanitarian ass...   \n",
      "6  What is the average water temperature for each...   \n",
      "7  Delete a program's outcome data with given SQL...   \n",
      "8  Find the total fare collected from passengers ...   \n",
      "9  What is the average property size in inclusive...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT salesperson_id, name, SUM(volume) ...  \n",
      "1  SQL: SELECT equipment_type, SUM(maintenance_fr...  \n",
      "2  SQL: SELECT COUNT(*) FROM marine_species WHERE...  \n",
      "3  SQL: SELECT trader_id, stock, SUM(price * quan...  \n",
      "4  SQL: SELECT type, cost FROM (SELECT type, cost...  \n",
      "5  SQL: SELECT SUM(spending) FROM defense.eu_huma...  \n",
      "6  SQL: SELECT SpeciesName, AVG(WaterTemp) as Avg...  \n",
      "7  SQL: DELETE FROM Program_Outcomes WHERE progra...  \n",
      "8  SQL: SELECT SUM(fare) FROM bus_routes WHERE ro...  \n",
      "9  SQL: SELECT AVG(Property_Size) FROM Inclusive_...  \n",
      "                                              prompt  \\\n",
      "0  What is the average explainability score of cr...   \n",
      "1  Delete all records of rural infrastructure pro...   \n",
      "2  How many accidents have been recorded for Spac...   \n",
      "3  What is the maximum quantity of seafood sold i...   \n",
      "4  What is the total budget for movies released b...   \n",
      "5  Add a new attorney named 'Oliver Martinez' wit...   \n",
      "6  Identify the top 2 plants with the highest CO2...   \n",
      "7  What is the total cost of all climate communic...   \n",
      "8  List all marine species with their conservatio...   \n",
      "9  What is the average number of publications per...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT AVG(explainability_score) FROM cre...  \n",
      "1  SQL: DELETE FROM rural_infrastructure WHERE co...  \n",
      "2  SQL: SELECT launch_provider, COUNT(*) FROM Acc...  \n",
      "3  SQL: SELECT MAX(quantity) FROM sales;Explanati...  \n",
      "4  SQL: SELECT SUM(budget) FROM Movies_Release_Ye...  \n",
      "5  SQL: INSERT INTO attorneys (attorney_name, att...  \n",
      "6  SQL: SELECT plant_name, SUM(co2_emission_per_t...  \n",
      "7  SQL: SELECT SUM(total_cost) FROM climate_commu...  \n",
      "8  SQL: SELECT name, conservation_status FROM mar...  \n",
      "9  SQL: SELECT organization, AVG(publications) as...  \n",
      "                                                 prompt  \\\n",
      "3900  What is the total number of tickets sold for a...   \n",
      "3901  What is the total revenue for the soccer team ...   \n",
      "3902  Identify the number of security incidents that...   \n",
      "3903  Identify the top 5 threat intelligence sources...   \n",
      "3904  What are the collective bargaining agreements ...   \n",
      "3905  How many vessels arrived in Brazil in July 202...   \n",
      "3906  What was the maximum cargo weight for vessels ...   \n",
      "3907  Find the top 3 regions with the highest water ...   \n",
      "3908  List all water sources located in California, ...   \n",
      "3909  What is the average bias score for each attrib...   \n",
      "\n",
      "                                             completion  \n",
      "3900  SQL: SELECT SUM(quantity) FROM tickets INNER J...  \n",
      "3901  SQL: SELECT SUM(tickets.quantity * games.price...  \n",
      "3902  SQL: SELECT COUNT(*) FROM incidents WHERE inci...  \n",
      "3903  SQL: SELECT source, SUM(incident_count) as tot...  \n",
      "3904  SQL: SELECT UnionName, ExpirationDate FROM CBA...  \n",
      "3905  SQL: SELECT COUNT(*) FROM vessel_performance W...  \n",
      "3906  SQL: SELECT MAX(cargo_weight) FROM vessels WHE...  \n",
      "3907  SQL: SELECT region, SUM(efforts) AS total_effo...  \n",
      "3908  SQL: SELECT * FROM water_sources WHERE locatio...  \n",
      "3909  SQL: SELECT algorithm, attribute, AVG(bias_sco...  \n",
      "                                              prompt  \\\n",
      "0  What is the total volume of timber sold by eac...   \n",
      "1  List all the unique equipment types and their ...   \n",
      "2  How many marine species are found in the South...   \n",
      "3  What is the total trade value and average pric...   \n",
      "4  Find the energy efficiency upgrades with the h...   \n",
      "5  What is the total spending on humanitarian ass...   \n",
      "6  What is the average water temperature for each...   \n",
      "7  Delete a program's outcome data with given SQL...   \n",
      "8  Find the total fare collected from passengers ...   \n",
      "9  What is the average property size in inclusive...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT salesperson_id, name, SUM(volume) ...  \n",
      "1  SQL: SELECT equipment_type, SUM(maintenance_fr...  \n",
      "2  SQL: SELECT COUNT(*) FROM marine_species WHERE...  \n",
      "3  SQL: SELECT trader_id, stock, SUM(price * quan...  \n",
      "4  SQL: SELECT type, cost FROM (SELECT type, cost...  \n",
      "5  SQL: SELECT SUM(spending) FROM defense.eu_huma...  \n",
      "6  SQL: SELECT SpeciesName, AVG(WaterTemp) as Avg...  \n",
      "7  SQL: DELETE FROM Program_Outcomes WHERE progra...  \n",
      "8  SQL: SELECT SUM(fare) FROM bus_routes WHERE ro...  \n",
      "9  SQL: SELECT AVG(Property_Size) FROM Inclusive_...  \n",
      "                                              prompt  \\\n",
      "0  What is the average explainability score of cr...   \n",
      "1  Delete all records of rural infrastructure pro...   \n",
      "2  How many accidents have been recorded for Spac...   \n",
      "3  What is the maximum quantity of seafood sold i...   \n",
      "4  What is the total budget for movies released b...   \n",
      "5  Add a new attorney named 'Oliver Martinez' wit...   \n",
      "6  Identify the top 2 plants with the highest CO2...   \n",
      "7  What is the total cost of all climate communic...   \n",
      "8  List all marine species with their conservatio...   \n",
      "9  What is the average number of publications per...   \n",
      "\n",
      "                                          completion  \n",
      "0  SQL: SELECT AVG(explainability_score) FROM cre...  \n",
      "1  SQL: DELETE FROM rural_infrastructure WHERE co...  \n",
      "2  SQL: SELECT launch_provider, COUNT(*) FROM Acc...  \n",
      "3  SQL: SELECT MAX(quantity) FROM sales;Explanati...  \n",
      "4  SQL: SELECT SUM(budget) FROM Movies_Release_Ye...  \n",
      "5  SQL: INSERT INTO attorneys (attorney_name, att...  \n",
      "6  SQL: SELECT plant_name, SUM(co2_emission_per_t...  \n",
      "7  SQL: SELECT SUM(total_cost) FROM climate_commu...  \n",
      "8  SQL: SELECT name, conservation_status FROM mar...  \n",
      "9  SQL: SELECT organization, AVG(publications) as...  \n",
      "                                                 prompt  \\\n",
      "3900  What is the total number of tickets sold for a...   \n",
      "3901  What is the total revenue for the soccer team ...   \n",
      "3902  Identify the number of security incidents that...   \n",
      "3903  Identify the top 5 threat intelligence sources...   \n",
      "3904  What are the collective bargaining agreements ...   \n",
      "3905  How many vessels arrived in Brazil in July 202...   \n",
      "3906  What was the maximum cargo weight for vessels ...   \n",
      "3907  Find the top 3 regions with the highest water ...   \n",
      "3908  List all water sources located in California, ...   \n",
      "3909  What is the average bias score for each attrib...   \n",
      "\n",
      "                                             completion  \n",
      "3900  SQL: SELECT SUM(quantity) FROM tickets INNER J...  \n",
      "3901  SQL: SELECT SUM(tickets.quantity * games.price...  \n",
      "3902  SQL: SELECT COUNT(*) FROM incidents WHERE inci...  \n",
      "3903  SQL: SELECT source, SUM(incident_count) as tot...  \n",
      "3904  SQL: SELECT UnionName, ExpirationDate FROM CBA...  \n",
      "3905  SQL: SELECT COUNT(*) FROM vessel_performance W...  \n",
      "3906  SQL: SELECT MAX(cargo_weight) FROM vessels WHE...  \n",
      "3907  SQL: SELECT region, SUM(efforts) AS total_effo...  \n",
      "3908  SQL: SELECT * FROM water_sources WHERE locatio...  \n",
      "3909  SQL: SELECT algorithm, attribute, AVG(bias_sco...  \n"
     ]
    }
   ],
   "source": [
    "from data.prepare import prepare_train,prepare_test_valid\n",
    "\n",
    "prepare_train()\n",
    "prepare_test_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLX Lib Model Loading-Training-Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdb8c4b9e0943feb459c2ce68e898d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Model load from mlx lib\n",
    "MODEL_ID=\"mlx-community/Llama-3.2-3B-Instruct-4bit\" # This is quantized model that not require quantization process.\n",
    "model, tokenizer = mlx_lm.load(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 17 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are competitors to Apache Kafka?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "==========\n",
      "There are several competitors to Apache Kafka in the market. Here are some of the notable ones:\n",
      "\n",
      "1. **Amazon Kinesis**: Amazon Kinesis is a fast, reliable, and scalable platform for real-time data processing and analysis. It's designed to handle large volumes of data from various sources and provides features like data streaming, processing, and analytics.\n",
      "\n",
      "2. **Google Cloud Pub/Sub**: Google Cloud Pub/Sub is a messaging system that allows publishers to send messages to subscribers. It's designed to handle large volumes of data and provides features like message queuing, routing, and transformation.\n",
      "\n",
      "3. **Microsoft Azure Event Grid**: Azure Event Grid is a cloud-based event bus that allows publishers to send events to subscribers. It's designed to handle large volumes of data and provides features like event routing, transformation, and filtering.\n",
      "\n",
      "4. **RabbitMQ**: RabbitMQ is a message broker that allows publishers to send messages to subscribers. It's designed to handle large volumes of data and provides features like message queuing, routing, and transformation.\n",
      "\n",
      "5. **Amazon SQS**: Amazon SQS (Simple Queue Service) is a message queue service that allows publishers to send messages to subscribers. It's designed to handle large volumes of data and provides features like message queuing, routing,\n",
      "==========\n",
      "Prompt: 42 tokens, 77.838 tokens-per-sec\n",
      "Generation: 256 tokens, 69.411 tokens-per-sec\n",
      "Peak memory: 1.874 GB\n"
     ]
    }
   ],
   "source": [
    "# Default Template\n",
    "user_content=\"What are competitors to Apache Kafka?\"\n",
    "\n",
    "if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    print(prompt)\n",
    "\n",
    "response = mlx_lm.generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 13 Feb 2025\n",
      "You are the support assistant for questions that are asked to you.\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are competitors to Apache Kafka?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "# Custom Template\n",
    "def custom_chat_template(messages):\n",
    "    \"\"\"Creates LLaMA 3.2 custom chat template.\"\"\"\n",
    "    chat_str = \"<|begin_of_text|>\"\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            chat_str += f\"<|start_header_id|>system<|end_header_id|>\\n\\n{msg['content']}\\n\\n<|eot_id|>\\n\"\n",
    "        elif msg[\"role\"] == \"user\":\n",
    "            chat_str += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_str += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\\n\"\n",
    "    chat_str += \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    return chat_str\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Cutting Knowledge Date: December 2023\\nToday Date: 13 Feb 2025\\nYou are the support assistant for questions that are asked to you.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are competitors to Apache Kafka?\"}\n",
    "]\n",
    "\n",
    "prompt = custom_chat_template(messages)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "Apache Kafka has several competitors in the market. Some of the notable ones are:\n",
      "\n",
      "1. Amazon Kinesis: Amazon Kinesis is a fast, reliable, and scalable stream processing service offered by Amazon Web Services (AWS). It's designed to handle large amounts of data in real-time and provides features like data processing, event-driven architecture, and real-time analytics.\n",
      "\n",
      "2. RabbitMQ: RabbitMQ is a message broker that supports multiple messaging patterns, including pub/sub, request/reply, and message queues. It's known for its high performance, reliability, and scalability.\n",
      "\n",
      "3. Apache Storm: Apache Storm is a distributed real-time computation system that can handle large amounts of data in real-time. It's designed to handle high-throughput and provides features like fault-tolerant, scalable, and real-time processing.\n",
      "\n",
      "4. Google Cloud Pub/Sub: Google Cloud Pub/Sub is a messaging service offered by Google Cloud Platform. It's designed to handle large amounts of data in real-time and provides features like high-throughput, low-latency, and scalable.\n",
      "\n",
      "5. Microsoft Azure Event Grid: Microsoft Azure Event Grid is a service offered by Microsoft Azure that allows you to receive notifications when data changes in your applications. It's designed to handle large amounts of data in real-time and\n",
      "==========\n",
      "Prompt: 56 tokens, 389.941 tokens-per-sec\n",
      "Generation: 256 tokens, 68.652 tokens-per-sec\n",
      "Peak memory: 1.875 GB\n"
     ]
    }
   ],
   "source": [
    "response = mlx_lm.generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mlx_lm.convert in mlx_lm:\n",
      "\n",
      "NAME\n",
      "    mlx_lm.convert - # Copyright © 2023-2024 Apple Inc.\n",
      "\n",
      "FUNCTIONS\n",
      "    configure_parser() -> argparse.ArgumentParser\n",
      "        Configures and returns the argument parser for the script.\n",
      "\n",
      "        Returns:\n",
      "            argparse.ArgumentParser: Configured argument parser.\n",
      "\n",
      "    main()\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/mlx_lm/convert.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlx_lm.convert as convert\n",
    "help(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2986f32ad654e5593ce32b110776190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 8.500 bits per weight.\n"
     ]
    }
   ],
   "source": [
    "# Quantizing huggingface model with mlx.convert api\n",
    "# Our model is already quantized beacuse of this we are going to quantize new huggingface model.\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    hf_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    q_bits=8,\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"convert.py\",\n",
    "    \"--hf-path\", str(args.hf_path),\n",
    "    \"--q-bits\", str(args.q_bits),\n",
    "    \"--quantize\",\n",
    "]\n",
    "\n",
    "convert.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune - QLORA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package mlx_lm:\n",
      "\n",
      "NAME\n",
      "    mlx_lm - # Copyright © 2023-2024 Apple Inc.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _version\n",
      "    cache_prompt\n",
      "    chat\n",
      "    convert\n",
      "    evaluate\n",
      "    fuse\n",
      "    generate\n",
      "    gguf\n",
      "    lora\n",
      "    manage\n",
      "    merge\n",
      "    models (package)\n",
      "    sample_utils\n",
      "    server\n",
      "    tokenizer_utils\n",
      "    tuner (package)\n",
      "    utils\n",
      "\n",
      "VERSION\n",
      "    0.21.2\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/mlx_lm/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mlx_lm)\n",
    "import argparse\n",
    "from mlx_lm import lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a68aac5bbd4c898e834f802162916d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.071% (2.294M/3212.750M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 2.246, Val took 21.200s\n",
      "Iter 10: Train loss 2.166, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 210.051, Trained Tokens 4394, Peak mem 10.933 GB\n",
      "Iter 20: Train loss 1.593, Learning Rate 1.000e-05, It/sec 0.552, Tokens/sec 220.859, Trained Tokens 8392, Peak mem 10.933 GB\n",
      "Iter 30: Train loss 1.168, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 224.946, Trained Tokens 12414, Peak mem 10.933 GB\n",
      "Iter 40: Train loss 0.922, Learning Rate 1.000e-05, It/sec 0.466, Tokens/sec 226.470, Trained Tokens 17272, Peak mem 11.278 GB\n",
      "Iter 50: Train loss 0.760, Learning Rate 1.000e-05, It/sec 0.538, Tokens/sec 235.703, Trained Tokens 21652, Peak mem 11.278 GB\n",
      "Iter 60: Train loss 0.688, Learning Rate 1.000e-05, It/sec 0.471, Tokens/sec 233.109, Trained Tokens 26597, Peak mem 11.278 GB\n",
      "Iter 70: Train loss 0.764, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 250.598, Trained Tokens 30889, Peak mem 11.278 GB\n",
      "Iter 80: Train loss 0.662, Learning Rate 1.000e-05, It/sec 0.498, Tokens/sec 228.089, Trained Tokens 35471, Peak mem 11.278 GB\n",
      "Iter 90: Train loss 0.747, Learning Rate 1.000e-05, It/sec 0.539, Tokens/sec 249.435, Trained Tokens 40103, Peak mem 11.278 GB\n",
      "Iter 100: Train loss 0.705, Learning Rate 1.000e-05, It/sec 0.618, Tokens/sec 252.376, Trained Tokens 44185, Peak mem 11.278 GB\n",
      "Iter 100: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.574, Tokens/sec 245.039, Trained Tokens 48455, Peak mem 11.278 GB\n",
      "Iter 120: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.497, Tokens/sec 251.274, Trained Tokens 53513, Peak mem 11.278 GB\n",
      "Iter 130: Train loss 0.714, Learning Rate 1.000e-05, It/sec 0.603, Tokens/sec 254.106, Trained Tokens 57729, Peak mem 11.278 GB\n",
      "Iter 140: Train loss 0.664, Learning Rate 1.000e-05, It/sec 0.541, Tokens/sec 246.672, Trained Tokens 62291, Peak mem 11.278 GB\n",
      "Iter 150: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.651, Tokens/sec 243.019, Trained Tokens 66025, Peak mem 11.278 GB\n",
      "Iter 160: Train loss 0.677, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 241.491, Trained Tokens 70615, Peak mem 11.278 GB\n",
      "Iter 170: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.600, Tokens/sec 245.477, Trained Tokens 74705, Peak mem 11.278 GB\n",
      "Iter 180: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.620, Tokens/sec 247.760, Trained Tokens 78701, Peak mem 11.278 GB\n",
      "Iter 190: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 245.494, Trained Tokens 83206, Peak mem 12.106 GB\n",
      "Iter 200: Val loss 0.656, Val took 23.333s\n",
      "Iter 200: Train loss 0.582, Learning Rate 1.000e-05, It/sec 4.609, Tokens/sec 2289.965, Trained Tokens 88174, Peak mem 12.106 GB\n",
      "Iter 200: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.588, Learning Rate 1.000e-05, It/sec 0.456, Tokens/sec 227.196, Trained Tokens 93160, Peak mem 12.106 GB\n",
      "Iter 220: Train loss 0.616, Learning Rate 1.000e-05, It/sec 0.445, Tokens/sec 224.953, Trained Tokens 98210, Peak mem 12.106 GB\n",
      "Iter 230: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.492, Tokens/sec 226.822, Trained Tokens 102820, Peak mem 12.106 GB\n",
      "Iter 240: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.510, Tokens/sec 241.390, Trained Tokens 107550, Peak mem 12.106 GB\n",
      "Iter 250: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.577, Tokens/sec 250.235, Trained Tokens 111888, Peak mem 12.106 GB\n",
      "Iter 260: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 244.398, Trained Tokens 115982, Peak mem 12.106 GB\n",
      "Iter 270: Train loss 0.617, Learning Rate 1.000e-05, It/sec 0.485, Tokens/sec 242.927, Trained Tokens 120990, Peak mem 12.106 GB\n",
      "Iter 280: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 250.170, Trained Tokens 125132, Peak mem 12.106 GB\n",
      "Iter 290: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.476, Tokens/sec 247.535, Trained Tokens 130330, Peak mem 12.106 GB\n",
      "Iter 300: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.654, Tokens/sec 251.402, Trained Tokens 134172, Peak mem 12.106 GB\n",
      "Iter 300: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.637, Tokens/sec 246.751, Trained Tokens 138046, Peak mem 12.106 GB\n",
      "Iter 320: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.657, Tokens/sec 248.500, Trained Tokens 141830, Peak mem 12.106 GB\n",
      "Iter 330: Train loss 0.645, Learning Rate 1.000e-05, It/sec 0.662, Tokens/sec 250.279, Trained Tokens 145608, Peak mem 12.106 GB\n",
      "Iter 340: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 246.601, Trained Tokens 150110, Peak mem 12.106 GB\n",
      "Iter 350: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.615, Tokens/sec 245.428, Trained Tokens 154098, Peak mem 12.106 GB\n",
      "Iter 360: Train loss 0.658, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 248.240, Trained Tokens 158090, Peak mem 12.106 GB\n",
      "Iter 370: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 241.383, Trained Tokens 162522, Peak mem 12.106 GB\n",
      "Iter 380: Train loss 0.619, Learning Rate 1.000e-05, It/sec 0.546, Tokens/sec 249.920, Trained Tokens 167098, Peak mem 12.106 GB\n",
      "Iter 390: Train loss 0.651, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 241.141, Trained Tokens 171502, Peak mem 12.106 GB\n",
      "Iter 400: Val loss 0.618, Val took 22.647s\n",
      "Iter 400: Train loss 0.581, Learning Rate 1.000e-05, It/sec 5.096, Tokens/sec 2473.407, Trained Tokens 176356, Peak mem 12.106 GB\n",
      "Iter 400: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 247.711, Trained Tokens 180372, Peak mem 12.106 GB\n",
      "Iter 420: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.527, Tokens/sec 245.219, Trained Tokens 185022, Peak mem 12.106 GB\n",
      "Iter 430: Train loss 0.630, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 244.814, Trained Tokens 189338, Peak mem 12.106 GB\n",
      "Iter 440: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 252.601, Trained Tokens 193520, Peak mem 12.106 GB\n",
      "Iter 450: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 237.136, Trained Tokens 197492, Peak mem 12.106 GB\n",
      "Iter 460: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.511, Tokens/sec 238.764, Trained Tokens 202167, Peak mem 12.106 GB\n",
      "Iter 470: Train loss 0.648, Learning Rate 1.000e-05, It/sec 0.477, Tokens/sec 194.447, Trained Tokens 206242, Peak mem 12.106 GB\n",
      "Iter 480: Train loss 0.601, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 254.598, Trained Tokens 210604, Peak mem 12.106 GB\n",
      "Iter 490: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.605, Tokens/sec 243.318, Trained Tokens 214624, Peak mem 12.106 GB\n",
      "Iter 500: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.625, Tokens/sec 248.248, Trained Tokens 218596, Peak mem 12.106 GB\n",
      "Iter 500: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 239.191, Trained Tokens 222840, Peak mem 12.106 GB\n",
      "Iter 520: Train loss 0.618, Learning Rate 1.000e-05, It/sec 0.547, Tokens/sec 232.257, Trained Tokens 227084, Peak mem 12.106 GB\n",
      "Iter 530: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 207.306, Trained Tokens 231422, Peak mem 12.106 GB\n",
      "Iter 540: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 238.023, Trained Tokens 235640, Peak mem 12.106 GB\n",
      "Iter 550: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 230.716, Trained Tokens 240140, Peak mem 12.106 GB\n",
      "Iter 560: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.488, Tokens/sec 226.623, Trained Tokens 244782, Peak mem 12.106 GB\n",
      "Iter 570: Train loss 0.607, Learning Rate 1.000e-05, It/sec 0.596, Tokens/sec 244.295, Trained Tokens 248884, Peak mem 12.106 GB\n",
      "Iter 580: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.611, Tokens/sec 233.846, Trained Tokens 252714, Peak mem 12.106 GB\n",
      "Iter 590: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.554, Tokens/sec 238.493, Trained Tokens 257022, Peak mem 12.106 GB\n",
      "Iter 600: Val loss 0.646, Val took 20.027s\n",
      "Iter 600: Train loss 0.607, Learning Rate 1.000e-05, It/sec 4.685, Tokens/sec 2164.955, Trained Tokens 261643, Peak mem 12.106 GB\n",
      "Iter 600: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.533, Tokens/sec 238.976, Trained Tokens 266129, Peak mem 12.106 GB\n",
      "Iter 620: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.534, Tokens/sec 243.171, Trained Tokens 270679, Peak mem 12.106 GB\n",
      "Iter 630: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.522, Tokens/sec 254.309, Trained Tokens 275553, Peak mem 12.106 GB\n",
      "Iter 640: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.663, Tokens/sec 241.131, Trained Tokens 279189, Peak mem 12.106 GB\n",
      "Iter 650: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.506, Tokens/sec 237.818, Trained Tokens 283889, Peak mem 12.106 GB\n",
      "Iter 660: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.857, Trained Tokens 287941, Peak mem 12.106 GB\n",
      "Iter 670: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.525, Tokens/sec 250.159, Trained Tokens 292709, Peak mem 12.106 GB\n",
      "Iter 680: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.451, Tokens/sec 250.664, Trained Tokens 298267, Peak mem 12.106 GB\n",
      "Iter 690: Train loss 0.598, Learning Rate 1.000e-05, It/sec 0.623, Tokens/sec 248.441, Trained Tokens 302257, Peak mem 12.106 GB\n",
      "Iter 700: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.443, Tokens/sec 239.808, Trained Tokens 307671, Peak mem 12.106 GB\n",
      "Iter 700: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 238.065, Trained Tokens 312193, Peak mem 12.106 GB\n",
      "Iter 720: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.599, Tokens/sec 253.881, Trained Tokens 316433, Peak mem 12.106 GB\n",
      "Iter 730: Train loss 0.627, Learning Rate 1.000e-05, It/sec 0.671, Tokens/sec 248.509, Trained Tokens 320139, Peak mem 12.106 GB\n",
      "Iter 740: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.614, Tokens/sec 249.034, Trained Tokens 324195, Peak mem 12.106 GB\n",
      "Iter 750: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.591, Tokens/sec 242.646, Trained Tokens 328303, Peak mem 12.106 GB\n",
      "Iter 760: Train loss 0.640, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 245.124, Trained Tokens 332967, Peak mem 12.106 GB\n",
      "Iter 770: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.644, Tokens/sec 245.625, Trained Tokens 336779, Peak mem 12.106 GB\n",
      "Iter 780: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.581, Tokens/sec 238.123, Trained Tokens 340877, Peak mem 12.106 GB\n",
      "Iter 790: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 243.045, Trained Tokens 344817, Peak mem 12.106 GB\n",
      "Iter 800: Val loss 0.658, Val took 21.425s\n",
      "Iter 800: Train loss 0.621, Learning Rate 1.000e-05, It/sec 4.594, Tokens/sec 2302.357, Trained Tokens 349829, Peak mem 12.106 GB\n",
      "Iter 800: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.499, Tokens/sec 253.345, Trained Tokens 354909, Peak mem 12.106 GB\n",
      "Iter 820: Train loss 0.625, Learning Rate 1.000e-05, It/sec 0.691, Tokens/sec 248.738, Trained Tokens 358509, Peak mem 12.106 GB\n",
      "Iter 830: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.587, Tokens/sec 242.270, Trained Tokens 362635, Peak mem 12.106 GB\n",
      "Iter 840: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 245.970, Trained Tokens 366704, Peak mem 12.106 GB\n",
      "Iter 850: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.627, Tokens/sec 248.142, Trained Tokens 370660, Peak mem 12.106 GB\n",
      "Iter 860: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 250.849, Trained Tokens 375546, Peak mem 12.106 GB\n",
      "Iter 870: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.583, Tokens/sec 242.341, Trained Tokens 379706, Peak mem 12.106 GB\n",
      "Iter 880: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.565, Tokens/sec 245.302, Trained Tokens 384050, Peak mem 12.106 GB\n",
      "Iter 890: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 249.611, Trained Tokens 388518, Peak mem 12.106 GB\n",
      "Iter 900: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 233.360, Trained Tokens 392796, Peak mem 12.106 GB\n",
      "Iter 900: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.575, Tokens/sec 235.257, Trained Tokens 396888, Peak mem 12.106 GB\n",
      "Iter 920: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.461, Tokens/sec 250.336, Trained Tokens 402324, Peak mem 12.106 GB\n",
      "Iter 930: Train loss 0.594, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.058, Trained Tokens 406358, Peak mem 12.106 GB\n",
      "Iter 940: Train loss 0.602, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 248.398, Trained Tokens 410742, Peak mem 12.106 GB\n",
      "Iter 950: Train loss 0.582, Learning Rate 1.000e-05, It/sec 0.553, Tokens/sec 253.048, Trained Tokens 415322, Peak mem 12.106 GB\n",
      "Iter 960: Train loss 0.667, Learning Rate 1.000e-05, It/sec 0.563, Tokens/sec 238.937, Trained Tokens 419566, Peak mem 12.106 GB\n",
      "Iter 970: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.524, Tokens/sec 245.174, Trained Tokens 424248, Peak mem 12.106 GB\n",
      "Iter 980: Train loss 0.635, Learning Rate 1.000e-05, It/sec 0.455, Tokens/sec 193.946, Trained Tokens 428514, Peak mem 12.106 GB\n",
      "Iter 990: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.601, Tokens/sec 253.396, Trained Tokens 432732, Peak mem 12.106 GB\n",
      "Iter 1000: Val loss 0.632, Val took 22.836s\n",
      "Iter 1000: Train loss 0.590, Learning Rate 1.000e-05, It/sec 3.968, Tokens/sec 1815.926, Trained Tokens 437308, Peak mem 12.106 GB\n",
      "Iter 1000: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0001000_adapters.safetensors.\n",
      "Saved final weights to adapters_llama_3b/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "# QLORA\n",
    "# Fine-Tune LLM Model- Since the model was quantized, it was applied to the lora without quantizing the model.\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    model=MODEL_ID,\n",
    "    data=\"./data/\",\n",
    "    train=True,\n",
    "    test=False,\n",
    "    batch_size=2,\n",
    "    num_layers=16,\n",
    "    iters=1000,\n",
    "    fine_tune_type=\"lora\",\n",
    "    resume_adapter_file=None,\n",
    "    adapter_path = \"adapters_llama_3b\",\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"lora.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--data\", str(args.data),\n",
    "    \"--train\",\n",
    "    \"--batch-size\", str(args.batch_size),\n",
    "    \"--num-layers\", str(args.num_layers),\n",
    "    \"--iters\", str(args.iters),\n",
    "    \"--fine-tune-type\", str(args.fine_tune_type),\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "    \"--seed\", str(args.seed)\n",
    "]\n",
    "\n",
    "lora.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL_ID=\"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "\n",
    "Loading datasets\n",
    "Training\n",
    "Trainable parameters: 0.071% (2.294M/3212.750M)\n",
    "Starting training..., iters: 1000\n",
    "Iter 1: Val loss 2.246, Val took 21.200s\n",
    "Iter 10: Train loss 2.166, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 210.051, Trained Tokens 4394, Peak mem 10.933 GB\n",
    "Iter 20: Train loss 1.593, Learning Rate 1.000e-05, It/sec 0.552, Tokens/sec 220.859, Trained Tokens 8392, Peak mem 10.933 GB\n",
    "Iter 30: Train loss 1.168, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 224.946, Trained Tokens 12414, Peak mem 10.933 GB\n",
    "Iter 40: Train loss 0.922, Learning Rate 1.000e-05, It/sec 0.466, Tokens/sec 226.470, Trained Tokens 17272, Peak mem 11.278 GB\n",
    "Iter 50: Train loss 0.760, Learning Rate 1.000e-05, It/sec 0.538, Tokens/sec 235.703, Trained Tokens 21652, Peak mem 11.278 GB\n",
    "Iter 60: Train loss 0.688, Learning Rate 1.000e-05, It/sec 0.471, Tokens/sec 233.109, Trained Tokens 26597, Peak mem 11.278 GB\n",
    "Iter 70: Train loss 0.764, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 250.598, Trained Tokens 30889, Peak mem 11.278 GB\n",
    "Iter 80: Train loss 0.662, Learning Rate 1.000e-05, It/sec 0.498, Tokens/sec 228.089, Trained Tokens 35471, Peak mem 11.278 GB\n",
    "Iter 90: Train loss 0.747, Learning Rate 1.000e-05, It/sec 0.539, Tokens/sec 249.435, Trained Tokens 40103, Peak mem 11.278 GB\n",
    "Iter 100: Train loss 0.705, Learning Rate 1.000e-05, It/sec 0.618, Tokens/sec 252.376, Trained Tokens 44185, Peak mem 11.278 GB\n",
    "Iter 100: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000100_adapters.safetensors.\n",
    "Iter 110: Train loss 0.686, Learning Rate 1.000e-05, It/sec 0.574, Tokens/sec 245.039, Trained Tokens 48455, Peak mem 11.278 GB\n",
    "Iter 120: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.497, Tokens/sec 251.274, Trained Tokens 53513, Peak mem 11.278 GB\n",
    "Iter 130: Train loss 0.714, Learning Rate 1.000e-05, It/sec 0.603, Tokens/sec 254.106, Trained Tokens 57729, Peak mem 11.278 GB\n",
    "Iter 140: Train loss 0.664, Learning Rate 1.000e-05, It/sec 0.541, Tokens/sec 246.672, Trained Tokens 62291, Peak mem 11.278 GB\n",
    "Iter 150: Train loss 0.621, Learning Rate 1.000e-05, It/sec 0.651, Tokens/sec 243.019, Trained Tokens 66025, Peak mem 11.278 GB\n",
    "Iter 160: Train loss 0.677, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 241.491, Trained Tokens 70615, Peak mem 11.278 GB\n",
    "Iter 170: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.600, Tokens/sec 245.477, Trained Tokens 74705, Peak mem 11.278 GB\n",
    "Iter 180: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.620, Tokens/sec 247.760, Trained Tokens 78701, Peak mem 11.278 GB\n",
    "Iter 190: Train loss 0.583, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 245.494, Trained Tokens 83206, Peak mem 12.106 GB\n",
    "Iter 200: Val loss 0.656, Val took 23.333s\n",
    "Iter 200: Train loss 0.582, Learning Rate 1.000e-05, It/sec 4.609, Tokens/sec 2289.965, Trained Tokens 88174, Peak mem 12.106 GB\n",
    "Iter 200: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000200_adapters.safetensors.\n",
    "Iter 210: Train loss 0.588, Learning Rate 1.000e-05, It/sec 0.456, Tokens/sec 227.196, Trained Tokens 93160, Peak mem 12.106 GB\n",
    "Iter 220: Train loss 0.616, Learning Rate 1.000e-05, It/sec 0.445, Tokens/sec 224.953, Trained Tokens 98210, Peak mem 12.106 GB\n",
    "Iter 230: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.492, Tokens/sec 226.822, Trained Tokens 102820, Peak mem 12.106 GB\n",
    "Iter 240: Train loss 0.642, Learning Rate 1.000e-05, It/sec 0.510, Tokens/sec 241.390, Trained Tokens 107550, Peak mem 12.106 GB\n",
    "Iter 250: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.577, Tokens/sec 250.235, Trained Tokens 111888, Peak mem 12.106 GB\n",
    "Iter 260: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 244.398, Trained Tokens 115982, Peak mem 12.106 GB\n",
    "Iter 270: Train loss 0.617, Learning Rate 1.000e-05, It/sec 0.485, Tokens/sec 242.927, Trained Tokens 120990, Peak mem 12.106 GB\n",
    "Iter 280: Train loss 0.643, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 250.170, Trained Tokens 125132, Peak mem 12.106 GB\n",
    "Iter 290: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.476, Tokens/sec 247.535, Trained Tokens 130330, Peak mem 12.106 GB\n",
    "Iter 300: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.654, Tokens/sec 251.402, Trained Tokens 134172, Peak mem 12.106 GB\n",
    "Iter 300: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000300_adapters.safetensors.\n",
    "Iter 310: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.637, Tokens/sec 246.751, Trained Tokens 138046, Peak mem 12.106 GB\n",
    "Iter 320: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.657, Tokens/sec 248.500, Trained Tokens 141830, Peak mem 12.106 GB\n",
    "Iter 330: Train loss 0.645, Learning Rate 1.000e-05, It/sec 0.662, Tokens/sec 250.279, Trained Tokens 145608, Peak mem 12.106 GB\n",
    "Iter 340: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 246.601, Trained Tokens 150110, Peak mem 12.106 GB\n",
    "Iter 350: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.615, Tokens/sec 245.428, Trained Tokens 154098, Peak mem 12.106 GB\n",
    "Iter 360: Train loss 0.658, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 248.240, Trained Tokens 158090, Peak mem 12.106 GB\n",
    "Iter 370: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 241.383, Trained Tokens 162522, Peak mem 12.106 GB\n",
    "Iter 380: Train loss 0.619, Learning Rate 1.000e-05, It/sec 0.546, Tokens/sec 249.920, Trained Tokens 167098, Peak mem 12.106 GB\n",
    "Iter 390: Train loss 0.651, Learning Rate 1.000e-05, It/sec 0.548, Tokens/sec 241.141, Trained Tokens 171502, Peak mem 12.106 GB\n",
    "Iter 400: Val loss 0.618, Val took 22.647s\n",
    "Iter 400: Train loss 0.581, Learning Rate 1.000e-05, It/sec 5.096, Tokens/sec 2473.407, Trained Tokens 176356, Peak mem 12.106 GB\n",
    "Iter 400: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000400_adapters.safetensors.\n",
    "Iter 410: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 247.711, Trained Tokens 180372, Peak mem 12.106 GB\n",
    "Iter 420: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.527, Tokens/sec 245.219, Trained Tokens 185022, Peak mem 12.106 GB\n",
    "Iter 430: Train loss 0.630, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 244.814, Trained Tokens 189338, Peak mem 12.106 GB\n",
    "Iter 440: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 252.601, Trained Tokens 193520, Peak mem 12.106 GB\n",
    "Iter 450: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.597, Tokens/sec 237.136, Trained Tokens 197492, Peak mem 12.106 GB\n",
    "Iter 460: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.511, Tokens/sec 238.764, Trained Tokens 202167, Peak mem 12.106 GB\n",
    "Iter 470: Train loss 0.648, Learning Rate 1.000e-05, It/sec 0.477, Tokens/sec 194.447, Trained Tokens 206242, Peak mem 12.106 GB\n",
    "Iter 480: Train loss 0.601, Learning Rate 1.000e-05, It/sec 0.584, Tokens/sec 254.598, Trained Tokens 210604, Peak mem 12.106 GB\n",
    "Iter 490: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.605, Tokens/sec 243.318, Trained Tokens 214624, Peak mem 12.106 GB\n",
    "Iter 500: Train loss 0.632, Learning Rate 1.000e-05, It/sec 0.625, Tokens/sec 248.248, Trained Tokens 218596, Peak mem 12.106 GB\n",
    "Iter 500: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000500_adapters.safetensors.\n",
    "Iter 510: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 239.191, Trained Tokens 222840, Peak mem 12.106 GB\n",
    "Iter 520: Train loss 0.618, Learning Rate 1.000e-05, It/sec 0.547, Tokens/sec 232.257, Trained Tokens 227084, Peak mem 12.106 GB\n",
    "Iter 530: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.478, Tokens/sec 207.306, Trained Tokens 231422, Peak mem 12.106 GB\n",
    "Iter 540: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.564, Tokens/sec 238.023, Trained Tokens 235640, Peak mem 12.106 GB\n",
    "Iter 550: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 230.716, Trained Tokens 240140, Peak mem 12.106 GB\n",
    "Iter 560: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.488, Tokens/sec 226.623, Trained Tokens 244782, Peak mem 12.106 GB\n",
    "Iter 570: Train loss 0.607, Learning Rate 1.000e-05, It/sec 0.596, Tokens/sec 244.295, Trained Tokens 248884, Peak mem 12.106 GB\n",
    "Iter 580: Train loss 0.626, Learning Rate 1.000e-05, It/sec 0.611, Tokens/sec 233.846, Trained Tokens 252714, Peak mem 12.106 GB\n",
    "Iter 590: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.554, Tokens/sec 238.493, Trained Tokens 257022, Peak mem 12.106 GB\n",
    "Iter 600: Val loss 0.646, Val took 20.027s\n",
    "Iter 600: Train loss 0.607, Learning Rate 1.000e-05, It/sec 4.685, Tokens/sec 2164.955, Trained Tokens 261643, Peak mem 12.106 GB\n",
    "Iter 600: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000600_adapters.safetensors.\n",
    "Iter 610: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.533, Tokens/sec 238.976, Trained Tokens 266129, Peak mem 12.106 GB\n",
    "Iter 620: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.534, Tokens/sec 243.171, Trained Tokens 270679, Peak mem 12.106 GB\n",
    "Iter 630: Train loss 0.622, Learning Rate 1.000e-05, It/sec 0.522, Tokens/sec 254.309, Trained Tokens 275553, Peak mem 12.106 GB\n",
    "Iter 640: Train loss 0.710, Learning Rate 1.000e-05, It/sec 0.663, Tokens/sec 241.131, Trained Tokens 279189, Peak mem 12.106 GB\n",
    "Iter 650: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.506, Tokens/sec 237.818, Trained Tokens 283889, Peak mem 12.106 GB\n",
    "Iter 660: Train loss 0.608, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.857, Trained Tokens 287941, Peak mem 12.106 GB\n",
    "Iter 670: Train loss 0.596, Learning Rate 1.000e-05, It/sec 0.525, Tokens/sec 250.159, Trained Tokens 292709, Peak mem 12.106 GB\n",
    "Iter 680: Train loss 0.565, Learning Rate 1.000e-05, It/sec 0.451, Tokens/sec 250.664, Trained Tokens 298267, Peak mem 12.106 GB\n",
    "Iter 690: Train loss 0.598, Learning Rate 1.000e-05, It/sec 0.623, Tokens/sec 248.441, Trained Tokens 302257, Peak mem 12.106 GB\n",
    "Iter 700: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.443, Tokens/sec 239.808, Trained Tokens 307671, Peak mem 12.106 GB\n",
    "Iter 700: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000700_adapters.safetensors.\n",
    "Iter 710: Train loss 0.567, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 238.065, Trained Tokens 312193, Peak mem 12.106 GB\n",
    "Iter 720: Train loss 0.646, Learning Rate 1.000e-05, It/sec 0.599, Tokens/sec 253.881, Trained Tokens 316433, Peak mem 12.106 GB\n",
    "Iter 730: Train loss 0.627, Learning Rate 1.000e-05, It/sec 0.671, Tokens/sec 248.509, Trained Tokens 320139, Peak mem 12.106 GB\n",
    "Iter 740: Train loss 0.587, Learning Rate 1.000e-05, It/sec 0.614, Tokens/sec 249.034, Trained Tokens 324195, Peak mem 12.106 GB\n",
    "Iter 750: Train loss 0.629, Learning Rate 1.000e-05, It/sec 0.591, Tokens/sec 242.646, Trained Tokens 328303, Peak mem 12.106 GB\n",
    "Iter 760: Train loss 0.640, Learning Rate 1.000e-05, It/sec 0.526, Tokens/sec 245.124, Trained Tokens 332967, Peak mem 12.106 GB\n",
    "Iter 770: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.644, Tokens/sec 245.625, Trained Tokens 336779, Peak mem 12.106 GB\n",
    "Iter 780: Train loss 0.609, Learning Rate 1.000e-05, It/sec 0.581, Tokens/sec 238.123, Trained Tokens 340877, Peak mem 12.106 GB\n",
    "Iter 790: Train loss 0.639, Learning Rate 1.000e-05, It/sec 0.617, Tokens/sec 243.045, Trained Tokens 344817, Peak mem 12.106 GB\n",
    "Iter 800: Val loss 0.658, Val took 21.425s\n",
    "Iter 800: Train loss 0.621, Learning Rate 1.000e-05, It/sec 4.594, Tokens/sec 2302.357, Trained Tokens 349829, Peak mem 12.106 GB\n",
    "Iter 800: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000800_adapters.safetensors.\n",
    "Iter 810: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.499, Tokens/sec 253.345, Trained Tokens 354909, Peak mem 12.106 GB\n",
    "Iter 820: Train loss 0.625, Learning Rate 1.000e-05, It/sec 0.691, Tokens/sec 248.738, Trained Tokens 358509, Peak mem 12.106 GB\n",
    "Iter 830: Train loss 0.610, Learning Rate 1.000e-05, It/sec 0.587, Tokens/sec 242.270, Trained Tokens 362635, Peak mem 12.106 GB\n",
    "Iter 840: Train loss 0.603, Learning Rate 1.000e-05, It/sec 0.604, Tokens/sec 245.970, Trained Tokens 366704, Peak mem 12.106 GB\n",
    "Iter 850: Train loss 0.566, Learning Rate 1.000e-05, It/sec 0.627, Tokens/sec 248.142, Trained Tokens 370660, Peak mem 12.106 GB\n",
    "Iter 860: Train loss 0.595, Learning Rate 1.000e-05, It/sec 0.513, Tokens/sec 250.849, Trained Tokens 375546, Peak mem 12.106 GB\n",
    "Iter 870: Train loss 0.611, Learning Rate 1.000e-05, It/sec 0.583, Tokens/sec 242.341, Trained Tokens 379706, Peak mem 12.106 GB\n",
    "Iter 880: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.565, Tokens/sec 245.302, Trained Tokens 384050, Peak mem 12.106 GB\n",
    "Iter 890: Train loss 0.614, Learning Rate 1.000e-05, It/sec 0.559, Tokens/sec 249.611, Trained Tokens 388518, Peak mem 12.106 GB\n",
    "Iter 900: Train loss 0.592, Learning Rate 1.000e-05, It/sec 0.545, Tokens/sec 233.360, Trained Tokens 392796, Peak mem 12.106 GB\n",
    "Iter 900: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0000900_adapters.safetensors.\n",
    "Iter 910: Train loss 0.589, Learning Rate 1.000e-05, It/sec 0.575, Tokens/sec 235.257, Trained Tokens 396888, Peak mem 12.106 GB\n",
    "Iter 920: Train loss 0.559, Learning Rate 1.000e-05, It/sec 0.461, Tokens/sec 250.336, Trained Tokens 402324, Peak mem 12.106 GB\n",
    "Iter 930: Train loss 0.594, Learning Rate 1.000e-05, It/sec 0.622, Tokens/sec 251.058, Trained Tokens 406358, Peak mem 12.106 GB\n",
    "Iter 940: Train loss 0.602, Learning Rate 1.000e-05, It/sec 0.567, Tokens/sec 248.398, Trained Tokens 410742, Peak mem 12.106 GB\n",
    "Iter 950: Train loss 0.582, Learning Rate 1.000e-05, It/sec 0.553, Tokens/sec 253.048, Trained Tokens 415322, Peak mem 12.106 GB\n",
    "Iter 960: Train loss 0.667, Learning Rate 1.000e-05, It/sec 0.563, Tokens/sec 238.937, Trained Tokens 419566, Peak mem 12.106 GB\n",
    "Iter 970: Train loss 0.564, Learning Rate 1.000e-05, It/sec 0.524, Tokens/sec 245.174, Trained Tokens 424248, Peak mem 12.106 GB\n",
    "Iter 980: Train loss 0.635, Learning Rate 1.000e-05, It/sec 0.455, Tokens/sec 193.946, Trained Tokens 428514, Peak mem 12.106 GB\n",
    "Iter 990: Train loss 0.600, Learning Rate 1.000e-05, It/sec 0.601, Tokens/sec 253.396, Trained Tokens 432732, Peak mem 12.106 GB\n",
    "Iter 1000: Val loss 0.632, Val took 22.836s\n",
    "Iter 1000: Train loss 0.590, Learning Rate 1.000e-05, It/sec 3.968, Tokens/sec 1815.926, Trained Tokens 437308, Peak mem 12.106 GB\n",
    "Iter 1000: Saved adapter weights to adapters_llama_3b/adapters.safetensors and adapters_llama_3b/0001000_adapters.safetensors.\n",
    "Saved final weights to adapters_llama_3b/adapters.safetensors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have two choice 1- Using the cell code , 2- Terminal code\n",
    "\"\"\"\n",
    "myenv/bin/python -m mlx_lm.lora \\\n",
    "  --model meta-llama/Llama-3.1-8B-Instruct \\\n",
    "  --data data \\\n",
    "  --train \\\n",
    "  --batch-size 1 \\\n",
    "  --num-layers 8 \\\n",
    "  --iters 600 \\\n",
    "  --fine-tune-type lora\n",
    "\"\"\"\n",
    "#Output:\n",
    "\"\"\"\n",
    "\n",
    "Loading pretrained model\n",
    "Fetching 11 files: 100%|████████████████████| 11/11 [00:00<00:00, 118300.88it/s]\n",
    "Loading datasets\n",
    "Training\n",
    "Trainable parameters: 0.042% (3.408M/8030.261M)\n",
    "Starting training..., iters: 600\n",
    "Iter 1: Val loss 2.041, Val took 1722.526s\n",
    "Iter 10: Train loss 1.944, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.451, Trained Tokens 1916, Peak mem 18.338 GB\n",
    "Iter 20: Train loss 1.203, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.662, Trained Tokens 4077, Peak mem 18.338 GB\n",
    "Iter 30: Train loss 0.893, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.769, Trained Tokens 6232, Peak mem 18.545 GB\n",
    "Iter 40: Train loss 0.679, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.675, Trained Tokens 8340, Peak mem 18.614 GB\n",
    "Iter 50: Train loss 0.560, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.572, Trained Tokens 10466, Peak mem 18.614 GB\n",
    "Iter 60: Train loss 0.690, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.537, Trained Tokens 12580, Peak mem 18.966 GB\n",
    "Iter 70: Train loss 0.613, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.811, Trained Tokens 14698, Peak mem 18.966 GB\n",
    "Iter 80: Train loss 0.544, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.827, Trained Tokens 16931, Peak mem 19.108 GB\n",
    "Iter 90: Train loss 0.542, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.823, Trained Tokens 19161, Peak mem 19.469 GB\n",
    "Iter 100: Train loss 0.572, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.688, Trained Tokens 21316, Peak mem 19.469 GB\n",
    "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
    "Iter 110: Train loss 0.570, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.323, Trained Tokens 23136, Peak mem 19.469 GB\n",
    "Iter 120: Train loss 0.569, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.672, Trained Tokens 25198, Peak mem 19.469 GB\n",
    "Iter 130: Train loss 0.558, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.794, Trained Tokens 27446, Peak mem 19.469 GB\n",
    "Iter 140: Train loss 0.593, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.597, Trained Tokens 29539, Peak mem 19.469 GB\n",
    "Iter 150: Train loss 0.514, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.728, Trained Tokens 31703, Peak mem 19.469 GB\n",
    "Iter 160: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.725, Trained Tokens 33645, Peak mem 19.469 GB\n",
    "Iter 170: Train loss 0.534, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.731, Trained Tokens 35593, Peak mem 19.469 GB\n",
    "Iter 180: Train loss 0.547, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.921, Trained Tokens 37819, Peak mem 19.469 GB\n",
    "Iter 190: Train loss 0.501, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.453, Trained Tokens 39666, Peak mem 19.469 GB\n",
    "Iter 200: Val loss 0.538, Val took 1592.634s\n",
    "Iter 200: Train loss 0.501, Learning Rate 1.000e-05, It/sec 0.116, Tokens/sec 28.023, Trained Tokens 42088, Peak mem 19.469 GB\n",
    "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
    "Iter 210: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.710, Trained Tokens 44153, Peak mem 19.469 GB\n",
    "Iter 220: Train loss 0.530, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.055, Trained Tokens 46397, Peak mem 19.469 GB\n",
    "Iter 230: Train loss 0.571, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.109, Trained Tokens 48594, Peak mem 20.605 GB\n",
    "Iter 240: Train loss 0.531, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.703, Trained Tokens 50617, Peak mem 20.605 GB\n",
    "Iter 250: Train loss 0.469, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.777, Trained Tokens 52803, Peak mem 20.605 GB\n",
    "Iter 260: Train loss 0.442, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.347, Trained Tokens 55329, Peak mem 20.605 GB\n",
    "Iter 270: Train loss 0.490, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.150, Trained Tokens 57556, Peak mem 20.605 GB\n",
    "Iter 280: Train loss 0.483, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.266, Trained Tokens 60085, Peak mem 20.914 GB\n",
    "Iter 290: Train loss 0.527, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.849, Trained Tokens 62247, Peak mem 20.914 GB\n",
    "Iter 300: Train loss 0.492, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.589, Trained Tokens 64205, Peak mem 20.914 GB\n",
    "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
    "Iter 310: Train loss 0.513, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.902, Trained Tokens 66314, Peak mem 20.914 GB\n",
    "Iter 320: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.007, Trained Tokens 68574, Peak mem 20.914 GB\n",
    "Iter 330: Train loss 0.521, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.760, Trained Tokens 70660, Peak mem 20.914 GB\n",
    "Iter 340: Train loss 0.541, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.534, Trained Tokens 72612, Peak mem 20.914 GB\n",
    "Iter 350: Train loss 0.519, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.869, Trained Tokens 74696, Peak mem 20.914 GB\n",
    "Iter 360: Train loss 0.504, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.757, Trained Tokens 76714, Peak mem 20.914 GB\n",
    "Iter 370: Train loss 0.487, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.765, Trained Tokens 78802, Peak mem 20.914 GB\n",
    "Iter 380: Train loss 0.542, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.315, Trained Tokens 80586, Peak mem 20.914 GB\n",
    "Iter 390: Train loss 0.521, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.463, Trained Tokens 83198, Peak mem 20.914 GB\n",
    "Iter 400: Val loss 0.514, Val took 1662.526s\n",
    "Iter 400: Train loss 0.477, Learning Rate 1.000e-05, It/sec 0.115, Tokens/sec 26.611, Trained Tokens 85519, Peak mem 20.914 GB\n",
    "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
    "Iter 410: Train loss 0.474, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.279, Trained Tokens 87424, Peak mem 20.914 GB\n",
    "Iter 420: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.011, Trained Tokens 89647, Peak mem 20.914 GB\n",
    "Iter 430: Train loss 0.504, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.087, Trained Tokens 92035, Peak mem 20.914 GB\n",
    "Iter 440: Train loss 0.495, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.046, Trained Tokens 94466, Peak mem 20.914 GB\n",
    "Iter 450: Train loss 0.503, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.672, Trained Tokens 96499, Peak mem 20.914 GB\n",
    "Iter 460: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.886, Trained Tokens 98635, Peak mem 20.914 GB\n",
    "Iter 470: Train loss 0.504, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.064, Trained Tokens 100886, Peak mem 20.914 GB\n",
    "Iter 480: Train loss 0.468, Learning Rate 1.000e-05, It/sec 0.012, Tokens/sec 2.955, Trained Tokens 103305, Peak mem 20.914 GB\n",
    "Iter 490: Train loss 0.515, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.808, Trained Tokens 105436, Peak mem 20.914 GB\n",
    "Iter 500: Train loss 0.495, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.799, Trained Tokens 107603, Peak mem 20.914 GB\n",
    "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
    "Iter 510: Train loss 0.584, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.285, Trained Tokens 110072, Peak mem 20.914 GB\n",
    "Iter 520: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.832, Trained Tokens 112253, Peak mem 20.914 GB\n",
    "Iter 530: Train loss 0.505, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.327, Trained Tokens 114805, Peak mem 20.914 GB\n",
    "Iter 540: Train loss 0.464, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.267, Trained Tokens 117348, Peak mem 20.914 GB\n",
    "Iter 550: Train loss 0.513, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.112, Trained Tokens 119569, Peak mem 20.914 GB\n",
    "Iter 560: Train loss 0.438, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 3.056, Trained Tokens 121756, Peak mem 20.914 GB\n",
    "Iter 570: Train loss 0.514, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 2.713, Trained Tokens 123820, Peak mem 20.914 GB\n",
    "Iter 580: Train loss 0.525, Learning Rate 1.000e-05, It/sec 0.013, Tokens/sec 3.026, Trained Tokens 126135, Peak mem 20.914 GB\n",
    "Iter 590: Train loss 0.520, Learning Rate 1.000e-05, It/sec 0.014, Tokens/sec 2.965, Trained Tokens 128242, Peak mem 20.914 GB\n",
    "Iter 600: Val loss 0.507, Val took 1597.140s\n",
    "Iter 600: Train loss 0.532, Learning Rate 1.000e-05, It/sec 0.089, Tokens/sec 16.756, Trained Tokens 130122, Peak mem 20.914 GB\n",
    "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
    "Saved final weights to adapters/adapters.safetensors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Testing\n",
      "Test loss 0.620, Test ppl 1.859.\n"
     ]
    }
   ],
   "source": [
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"lora.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--data\", str(args.data),\n",
    "    \"--test\",\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "]\n",
    "\n",
    "lora.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the model decides between about 1,859 different words for each word. This is a good result for language models (Usually PPL < 10 is considered good, PPL ~ 2 is close to perfect).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained vs Fine Tuned LLM Output Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "I don't have any information about transactions or customers from the 'Africa' region. I'm a text-based AI assistant and do not have access to any specific data or databases. If you could provide more context or clarify what you are referring to, I'll do my best to help.\n",
      "==========\n",
      "Prompt: 61 tokens, 71.435 tokens-per-sec\n",
      "Generation: 60 tokens, 68.330 tokens-per-sec\n",
      "Peak memory: 1.875 GB\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "\n",
    "messages_base = [\n",
    "    {\"role\": \"system\", \"content\": \"Cutting Knowledge Date: December 2023\\nToday Date: 14 Feb 2025\\nYou are the support assistant for questions that are asked to you.\"},\n",
    "    {\"role\": \"user\", \"content\": \"List all transactions and customers from the 'Africa' region.\"}\n",
    "]\n",
    "\n",
    "prompt_base = custom_chat_template(messages_base)\n",
    "response = mlx_lm.generate(model, tokenizer, prompt=prompt_base, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "\n",
      "\n",
      "SQL: SELECT transactions.id, customers.name, transactions.amount, customers.country FROM transactions JOIN customers ON transactions.customer_id = customers.id WHERE customers.region = 'Africa';\n",
      "==========\n",
      "Prompt: 62 tokens, 160.189 tokens-per-sec\n",
      "Generation: 34 tokens, 51.437 tokens-per-sec\n",
      "Peak memory: 3.686 GB\n"
     ]
    }
   ],
   "source": [
    "# Fine Tuned Model\n",
    "\n",
    "import mlx_lm.generate as generate\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "messages_ft = [\n",
    "    {\"role\": \"system\", \"content\": \"Cutting Knowledge Date: December 2023\\nToday Date: 14 Feb 2025\\nYou are the support assistant for questions that are asked to you.\"},\n",
    "    {\"role\": \"user\", \"content\": \"List all transactions and customers from the 'Africa' region.\"}\n",
    "]\n",
    "\n",
    "prompt_ft = custom_chat_template(messages_ft)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    model=MODEL_ID,\n",
    "    adapter_path = \"./adapters_llama_3b/\",\n",
    "    prompt = prompt_ft,\n",
    "    max_tokens = 500,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"generate.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--ignore-chat-template\",\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "    \"--prompt\",str(args.prompt),\n",
    "    \"--max-tokens\",str(args.max_tokens),\n",
    "    \"--verbose\",str(args.verbose)\n",
    "]\n",
    "\n",
    "generate.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse Adapters to Model for Building New LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mlx_lm.fuse in mlx_lm:\n",
      "\n",
      "NAME\n",
      "    mlx_lm.fuse\n",
      "\n",
      "FUNCTIONS\n",
      "    main() -> None\n",
      "\n",
      "    parse_arguments() -> argparse.Namespace\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/mlx_lm/fuse.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import fuse\n",
    "\n",
    "help(fuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff71ff33041c4997a8d8212bb04c9881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-quantizing model\n",
      "Converting to GGUF format\n",
      "Converted GGUF model saved as: sql_agent/ggml-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    model=MODEL_ID,\n",
    "    adapter_path = \"./adapters_llama_3b/\",\n",
    "    prompt = prompt_ft,\n",
    "    max_tokens = 500,\n",
    "    verbose = True,\n",
    "    save_path = \"./sql_agent/\",\n",
    "    gguf_path = \"./sql_agent/gguf_model/ggml-model-sql-agent.gguf\"\n",
    ")\n",
    "\n",
    "# Set args with sys.argv \n",
    "sys.argv = [\n",
    "    \"fuse.py\",\n",
    "    \"--model\", str(args.model),\n",
    "    \"--adapter-path\", str(args.adapter_path),\n",
    "    \"--de-quantize\", #Its because dequantize models not supported with gguf format.\n",
    "    \"--save-path\", str(args.save_path),\n",
    "    \"--export-gguf\",\n",
    "    #\"--gguf-path\", str(args.gguf_path)\n",
    "]\n",
    "\n",
    "fuse.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model with OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package ollama:\n",
      "\n",
      "NAME\n",
      "    ollama\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _client\n",
      "    _types\n",
      "    _utils\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        ollama._types.RequestError\n",
      "        ollama._types.ResponseError\n",
      "    ollama._client.BaseClient(builtins.object)\n",
      "        ollama._client.AsyncClient\n",
      "        ollama._client.Client\n",
      "    ollama._types.BaseGenerateResponse(ollama._types.SubscriptableBaseModel)\n",
      "        ollama._types.ChatResponse\n",
      "        ollama._types.EmbedResponse\n",
      "        ollama._types.GenerateResponse\n",
      "    ollama._types.SubscriptableBaseModel(pydantic.main.BaseModel)\n",
      "        ollama._types.EmbeddingsResponse\n",
      "        ollama._types.ListResponse\n",
      "        ollama._types.Message\n",
      "        ollama._types.Options\n",
      "        ollama._types.ProcessResponse\n",
      "        ollama._types.ShowResponse\n",
      "        ollama._types.StatusResponse\n",
      "            ollama._types.ProgressResponse\n",
      "        ollama._types.Tool\n",
      "    pydantic.main.BaseModel(builtins.object)\n",
      "        ollama._types.Image\n",
      "\n",
      "    class AsyncClient(BaseClient)\n",
      "     |  AsyncClient(host: Optional[str] = None, **kwargs) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      AsyncClient\n",
      "     |      BaseClient\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, host: Optional[str] = None, **kwargs) -> None\n",
      "     |      Creates a httpx client. Default parameters are the same as those defined in httpx\n",
      "     |      except for the following:\n",
      "     |      - `follow_redirects`: True\n",
      "     |      - `timeout`: None\n",
      "     |      `kwargs` are passed to the httpx client.\n",
      "     |\n",
      "     |  async chat(self, model: str = '', messages: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Message]]] = None, *, tools: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Tool, Callable]]] = None, stream: bool = False, format: Union[Literal['', 'json'], Dict[str, Any], NoneType] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> Union[ollama._types.ChatResponse, collections.abc.AsyncIterator[ollama._types.ChatResponse]]\n",
      "     |      Create a chat response using the requested model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |        tools:\n",
      "     |          A JSON schema as a dict, an Ollama Tool or a Python Function.\n",
      "     |          Python functions need to follow Google style docstrings to be converted to an Ollama Tool.\n",
      "     |          For more information, see: https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\n",
      "     |        stream: Whether to stream the response.\n",
      "     |        format: The format of the response.\n",
      "     |\n",
      "     |      Example:\n",
      "     |        def add_two_numbers(a: int, b: int) -> int:\n",
      "     |          '''\n",
      "     |          Add two numbers together.\n",
      "     |\n",
      "     |          Args:\n",
      "     |            a: First number to add\n",
      "     |            b: Second number to add\n",
      "     |\n",
      "     |          Returns:\n",
      "     |            int: The sum of a and b\n",
      "     |          '''\n",
      "     |          return a + b\n",
      "     |\n",
      "     |        await client.chat(model='llama3.2', tools=[add_two_numbers], messages=[...])\n",
      "     |\n",
      "     |      Raises `RequestError` if a model is not provided.\n",
      "     |\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\n",
      "     |\n",
      "     |  async copy(self, source: str, destination: str) -> ollama._types.StatusResponse\n",
      "     |\n",
      "     |  async create(self, model: str, quantize: Optional[str] = None, from_: Optional[str] = None, files: Optional[Dict[str, str]] = None, adapters: Optional[Dict[str, str]] = None, template: Optional[str] = None, license: Union[str, List[str], NoneType] = None, system: Optional[str] = None, parameters: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, messages: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Message]]] = None, *, stream: bool = False) -> Union[ollama._types.ProgressResponse, collections.abc.AsyncIterator[ollama._types.ProgressResponse]]\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n",
      "     |\n",
      "     |  async create_blob(self, path: Union[str, pathlib.Path]) -> str\n",
      "     |\n",
      "     |  async delete(self, model: str) -> ollama._types.StatusResponse\n",
      "     |\n",
      "     |  async embed(self, model: str = '', input: Union[str, Sequence[str]] = '', truncate: Optional[bool] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> ollama._types.EmbedResponse\n",
      "     |\n",
      "     |  async embeddings(self, model: str = '', prompt: Optional[str] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> ollama._types.EmbeddingsResponse\n",
      "     |      Deprecated in favor of `embed`.\n",
      "     |\n",
      "     |  async generate(self, model: str = '', prompt: Optional[str] = None, suffix: Optional[str] = None, *, system: Optional[str] = None, template: Optional[str] = None, context: Optional[Sequence[int]] = None, stream: bool = False, raw: Optional[bool] = None, format: Union[Literal['', 'json'], Dict[str, Any], NoneType] = None, images: Optional[Sequence[Union[str, bytes]]] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> Union[ollama._types.GenerateResponse, collections.abc.AsyncIterator[ollama._types.GenerateResponse]]\n",
      "     |      Create a response using the requested model.\n",
      "     |\n",
      "     |      Raises `RequestError` if a model is not provided.\n",
      "     |\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `GenerateResponse` if `stream` is `False`, otherwise returns an asynchronous `GenerateResponse` generator.\n",
      "     |\n",
      "     |  async list(self) -> ollama._types.ListResponse\n",
      "     |\n",
      "     |  async ps(self) -> ollama._types.ProcessResponse\n",
      "     |\n",
      "     |  async pull(self, model: str, *, insecure: bool = False, stream: bool = False) -> Union[ollama._types.ProgressResponse, collections.abc.AsyncIterator[ollama._types.ProgressResponse]]\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n",
      "     |\n",
      "     |  async push(self, model: str, *, insecure: bool = False, stream: bool = False) -> Union[ollama._types.ProgressResponse, collections.abc.AsyncIterator[ollama._types.ProgressResponse]]\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n",
      "     |\n",
      "     |  async show(self, model: str) -> ollama._types.ShowResponse\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseClient:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class ChatResponse(BaseGenerateResponse)\n",
      "     |  ChatResponse(*, model: Optional[str] = None, created_at: Optional[str] = None, done: Optional[bool] = None, done_reason: Optional[str] = None, total_duration: Optional[int] = None, load_duration: Optional[int] = None, prompt_eval_count: Optional[int] = None, prompt_eval_duration: Optional[int] = None, eval_count: Optional[int] = None, eval_duration: Optional[int] = None, message: ollama._types.Message) -> None\n",
      "     |\n",
      "     |  Response returned by chat requests.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      ChatResponse\n",
      "     |      BaseGenerateResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'message': <class 'ollama._types.Message'>}\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'created_at': FieldInfo(annotation=Union[str, N...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"ChatResponse\", validat...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, model: Optional[str] = None, crea... = ...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class Client(BaseClient)\n",
      "     |  Client(host: Optional[str] = None, **kwargs) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Client\n",
      "     |      BaseClient\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, host: Optional[str] = None, **kwargs) -> None\n",
      "     |      Creates a httpx client. Default parameters are the same as those defined in httpx\n",
      "     |      except for the following:\n",
      "     |      - `follow_redirects`: True\n",
      "     |      - `timeout`: None\n",
      "     |      `kwargs` are passed to the httpx client.\n",
      "     |\n",
      "     |  chat(self, model: str = '', messages: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Message]]] = None, *, tools: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Tool, Callable]]] = None, stream: bool = False, format: Union[Literal['', 'json'], Dict[str, Any], NoneType] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> Union[ollama._types.ChatResponse, collections.abc.Iterator[ollama._types.ChatResponse]]\n",
      "     |      Create a chat response using the requested model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |        tools:\n",
      "     |          A JSON schema as a dict, an Ollama Tool or a Python Function.\n",
      "     |          Python functions need to follow Google style docstrings to be converted to an Ollama Tool.\n",
      "     |          For more information, see: https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\n",
      "     |        stream: Whether to stream the response.\n",
      "     |        format: The format of the response.\n",
      "     |\n",
      "     |      Example:\n",
      "     |        def add_two_numbers(a: int, b: int) -> int:\n",
      "     |          '''\n",
      "     |          Add two numbers together.\n",
      "     |\n",
      "     |          Args:\n",
      "     |            a: First number to add\n",
      "     |            b: Second number to add\n",
      "     |\n",
      "     |          Returns:\n",
      "     |            int: The sum of a and b\n",
      "     |          '''\n",
      "     |          return a + b\n",
      "     |\n",
      "     |        client.chat(model='llama3.2', tools=[add_two_numbers], messages=[...])\n",
      "     |\n",
      "     |      Raises `RequestError` if a model is not provided.\n",
      "     |\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\n",
      "     |\n",
      "     |  copy(self, source: str, destination: str) -> ollama._types.StatusResponse\n",
      "     |\n",
      "     |  create(self, model: str, quantize: Optional[str] = None, from_: Optional[str] = None, files: Optional[Dict[str, str]] = None, adapters: Optional[Dict[str, str]] = None, template: Optional[str] = None, license: Union[str, List[str], NoneType] = None, system: Optional[str] = None, parameters: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, messages: Optional[Sequence[Union[Mapping[str, Any], ollama._types.Message]]] = None, *, stream: bool = False) -> Union[ollama._types.ProgressResponse, collections.abc.Iterator[ollama._types.ProgressResponse]]\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n",
      "     |\n",
      "     |  create_blob(self, path: Union[str, pathlib.Path]) -> str\n",
      "     |\n",
      "     |  delete(self, model: str) -> ollama._types.StatusResponse\n",
      "     |\n",
      "     |  embed(self, model: str = '', input: Union[str, Sequence[str]] = '', truncate: Optional[bool] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> ollama._types.EmbedResponse\n",
      "     |\n",
      "     |  embeddings(self, model: str = '', prompt: Optional[str] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> ollama._types.EmbeddingsResponse\n",
      "     |      Deprecated in favor of `embed`.\n",
      "     |\n",
      "     |  generate(self, model: str = '', prompt: Optional[str] = None, suffix: Optional[str] = None, *, system: Optional[str] = None, template: Optional[str] = None, context: Optional[Sequence[int]] = None, stream: bool = False, raw: Optional[bool] = None, format: Union[Literal['', 'json'], Dict[str, Any], NoneType] = None, images: Optional[Sequence[Union[str, bytes]]] = None, options: Union[Mapping[str, Any], ollama._types.Options, NoneType] = None, keep_alive: Union[float, str, NoneType] = None) -> Union[ollama._types.GenerateResponse, collections.abc.Iterator[ollama._types.GenerateResponse]]\n",
      "     |      Create a response using the requested model.\n",
      "     |\n",
      "     |      Raises `RequestError` if a model is not provided.\n",
      "     |\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\n",
      "     |\n",
      "     |  list(self) -> ollama._types.ListResponse\n",
      "     |\n",
      "     |  ps(self) -> ollama._types.ProcessResponse\n",
      "     |\n",
      "     |  pull(self, model: str, *, insecure: bool = False, stream: bool = False) -> Union[ollama._types.ProgressResponse, collections.abc.Iterator[ollama._types.ProgressResponse]]\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n",
      "     |\n",
      "     |  push(self, model: str, *, insecure: bool = False, stream: bool = False) -> Union[ollama._types.ProgressResponse, collections.abc.Iterator[ollama._types.ProgressResponse]]\n",
      "     |      Raises `ResponseError` if the request could not be fulfilled.\n",
      "     |\n",
      "     |      Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n",
      "     |\n",
      "     |  show(self, model: str) -> ollama._types.ShowResponse\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseClient:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "    class EmbedResponse(BaseGenerateResponse)\n",
      "     |  EmbedResponse(*, model: Optional[str] = None, created_at: Optional[str] = None, done: Optional[bool] = None, done_reason: Optional[str] = None, total_duration: Optional[int] = None, load_duration: Optional[int] = None, prompt_eval_count: Optional[int] = None, prompt_eval_duration: Optional[int] = None, eval_count: Optional[int] = None, eval_duration: Optional[int] = None, embeddings: Sequence[Sequence[float]]) -> None\n",
      "     |\n",
      "     |  Response returned by embed requests.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      EmbedResponse\n",
      "     |      BaseGenerateResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'embeddings': typing.Sequence[typing.Sequence[float...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.EmbedResponse...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'created_at': FieldInfo(annotation=Union[str, N...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"EmbedResponse\", valida...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, model: Optional[str] = None, crea..., e...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class EmbeddingsResponse(SubscriptableBaseModel)\n",
      "     |  EmbeddingsResponse(*, embedding: Sequence[float]) -> None\n",
      "     |\n",
      "     |  Response returned by embeddings requests.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      EmbeddingsResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'embedding': typing.Sequence[float]}\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.EmbeddingsRes...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'embedding': FieldInfo(annotation=Sequence[floa...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"EmbeddingsResponse\", v...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, embedding: Sequence[float]) -> None>\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class GenerateResponse(BaseGenerateResponse)\n",
      "     |  GenerateResponse(*, model: Optional[str] = None, created_at: Optional[str] = None, done: Optional[bool] = None, done_reason: Optional[str] = None, total_duration: Optional[int] = None, load_duration: Optional[int] = None, prompt_eval_count: Optional[int] = None, prompt_eval_duration: Optional[int] = None, eval_count: Optional[int] = None, eval_duration: Optional[int] = None, response: str, context: Optional[Sequence[int]] = None) -> None\n",
      "     |\n",
      "     |  Response returned by generate requests.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      GenerateResponse\n",
      "     |      BaseGenerateResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'context': typing.Optional[typing.Sequence[int]], '...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.GenerateRespo...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'context': FieldInfo(annotation=Union[Sequence[...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"GenerateResponse\", val...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, model: Optional[str] = None, crea...con...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class Image(pydantic.main.BaseModel)\n",
      "     |  Image(*, value: Union[str, bytes, pathlib.Path]) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Image\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  serialize_model(self)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'value': typing.Union[str, bytes, pathlib.Path]}\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.Image'>, 'con...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'value': FieldInfo(annotation=Union[str, bytes,...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Function(\n",
      "     |      Fu...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"Image\", validator=Mode...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, value: Union[str, bytes, pathlib.Path])...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class ListResponse(SubscriptableBaseModel)\n",
      "     |  ListResponse(*, models: Sequence[ollama._types.ListResponse.Model]) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      ListResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  Model = <class 'ollama._types.ListResponse.Model'>\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'models': typing.Sequence[ollama._types.ListRespons...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'models': FieldInfo(annotation=Sequence[ListRes...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"ListResponse\", validat...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, models: Sequence[ollama._types.ListResp...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class Message(SubscriptableBaseModel)\n",
      "     |  Message(*, role: Literal['user', 'assistant', 'system', 'tool'], content: Optional[str] = None, images: Optional[Sequence[ollama._types.Image]] = None, tool_calls: Optional[Sequence[ollama._types.Message.ToolCall]] = None) -> None\n",
      "     |\n",
      "     |  Chat message.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Message\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  ToolCall = <class 'ollama._types.Message.ToolCall'>\n",
      "     |      Model tool calls.\n",
      "     |\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'content': typing.Optional[str], 'images': typing.O...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'content': FieldInfo(annotation=Union[str, None...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"Message\", validator=Mo...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, role: Literal['user', 'assistant'...oll...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class Options(SubscriptableBaseModel)\n",
      "     |  Options(*, numa: Optional[bool] = None, num_ctx: Optional[int] = None, num_batch: Optional[int] = None, num_gpu: Optional[int] = None, main_gpu: Optional[int] = None, low_vram: Optional[bool] = None, f16_kv: Optional[bool] = None, logits_all: Optional[bool] = None, vocab_only: Optional[bool] = None, use_mmap: Optional[bool] = None, use_mlock: Optional[bool] = None, embedding_only: Optional[bool] = None, num_thread: Optional[int] = None, num_keep: Optional[int] = None, seed: Optional[int] = None, num_predict: Optional[int] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, tfs_z: Optional[float] = None, typical_p: Optional[float] = None, repeat_last_n: Optional[int] = None, temperature: Optional[float] = None, repeat_penalty: Optional[float] = None, presence_penalty: Optional[float] = None, frequency_penalty: Optional[float] = None, mirostat: Optional[int] = None, mirostat_tau: Optional[float] = None, mirostat_eta: Optional[float] = None, penalize_newline: Optional[bool] = None, stop: Optional[Sequence[str]] = None) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Options\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'embedding_only': typing.Optional[bool], 'f16_kv': ...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.Options'>, 'c...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'embedding_only': FieldInfo(annotation=Union[bo...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"Options\", validator=Mo...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, numa: Optional[bool] = None, num_...e, ...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class ProcessResponse(SubscriptableBaseModel)\n",
      "     |  ProcessResponse(*, models: Sequence[ollama._types.ProcessResponse.Model]) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      ProcessResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  Model = <class 'ollama._types.ProcessResponse.Model'>\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'models': typing.Sequence[ollama._types.ProcessResp...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'models': FieldInfo(annotation=Sequence[Process...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"ProcessResponse\", vali...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, models: Sequence[ollama._types.ProcessR...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class ProgressResponse(StatusResponse)\n",
      "     |  ProgressResponse(*, status: Optional[str] = None, completed: Optional[int] = None, total: Optional[int] = None, digest: Optional[str] = None) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      ProgressResponse\n",
      "     |      StatusResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'completed': typing.Optional[int], 'digest': typing...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.ProgressRespo...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'completed': FieldInfo(annotation=Union[int, No...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"ProgressResponse\", val...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, status: Optional[str] = None, com...t] ...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class RequestError(builtins.Exception)\n",
      "     |  RequestError(error: str)\n",
      "     |\n",
      "     |  Common class for request errors.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      RequestError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, error: str)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(...)\n",
      "     |\n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  add_note(...)\n",
      "     |      Exception.add_note(note) --\n",
      "     |      add a note to the exception\n",
      "     |\n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |\n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |\n",
      "     |  __dict__\n",
      "     |\n",
      "     |  __suppress_context__\n",
      "     |\n",
      "     |  __traceback__\n",
      "     |\n",
      "     |  args\n",
      "\n",
      "    class ResponseError(builtins.Exception)\n",
      "     |  ResponseError(error: str, status_code: int = -1)\n",
      "     |\n",
      "     |  Common class for response errors.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      ResponseError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __init__(self, error: str, status_code: int = -1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |\n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |\n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |\n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(...)\n",
      "     |\n",
      "     |  add_note(...)\n",
      "     |      Exception.add_note(note) --\n",
      "     |      add a note to the exception\n",
      "     |\n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |\n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |\n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |\n",
      "     |  __dict__\n",
      "     |\n",
      "     |  __suppress_context__\n",
      "     |\n",
      "     |  __traceback__\n",
      "     |\n",
      "     |  args\n",
      "\n",
      "    class ShowResponse(SubscriptableBaseModel)\n",
      "     |  ShowResponse(*, modified_at: Optional[datetime.datetime] = None, template: Optional[str] = None, modelfile: Optional[str] = None, license: Optional[str] = None, details: Optional[ollama._types.ModelDetails] = None, model_info: Optional[Mapping[str, Any]], parameters: Optional[str] = None) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      ShowResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'details': typing.Optional[ollama._types.ModelDetai...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.ShowResponse'...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'details': FieldInfo(annotation=Union[ModelDeta...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"ShowResponse\", validat...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, modified_at: Optional[datetime.da...Any...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class StatusResponse(SubscriptableBaseModel)\n",
      "     |  StatusResponse(*, status: Optional[str] = None) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      StatusResponse\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'status': typing.Optional[str]}\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.StatusRespons...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'status': FieldInfo(annotation=Union[str, NoneT...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"StatusResponse\", valid...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, status: Optional[str] = None) -> None>\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "    class Tool(SubscriptableBaseModel)\n",
      "     |  Tool(*, type: Optional[Literal['function']] = 'function', function: Optional[ollama._types.Tool.Function] = None) -> None\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      Tool\n",
      "     |      SubscriptableBaseModel\n",
      "     |      pydantic.main.BaseModel\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  Function = <class 'ollama._types.Tool.Function'>\n",
      "     |\n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |\n",
      "     |  __annotations__ = {'function': typing.Optional[ollama._types.Tool.Func...\n",
      "     |\n",
      "     |  __class_vars__ = set()\n",
      "     |\n",
      "     |  __private_attributes__ = {}\n",
      "     |\n",
      "     |  __pydantic_complete__ = True\n",
      "     |\n",
      "     |  __pydantic_computed_fields__ = {}\n",
      "     |\n",
      "     |  __pydantic_core_schema__ = {'cls': <class 'ollama._types.Tool'>, 'conf...\n",
      "     |\n",
      "     |  __pydantic_custom_init__ = False\n",
      "     |\n",
      "     |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      "     |\n",
      "     |  __pydantic_fields__ = {'function': FieldInfo(annotation=Union[Tool.Fun...\n",
      "     |\n",
      "     |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      "     |\n",
      "     |  __pydantic_parent_namespace__ = None\n",
      "     |\n",
      "     |  __pydantic_post_init__ = None\n",
      "     |\n",
      "     |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      "     |      Model...\n",
      "     |\n",
      "     |  __pydantic_validator__ = SchemaValidator(title=\"Tool\", validator=Model...\n",
      "     |\n",
      "     |  __signature__ = <Signature (*, type: Optional[Literal['function'...nal...\n",
      "     |\n",
      "     |  model_config = {}\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __contains__(self, key: str) -> bool\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> 'nonexistent' in msg\n",
      "     |      False\n",
      "     |      >>> 'role' in msg\n",
      "     |      True\n",
      "     |      >>> 'content' in msg\n",
      "     |      False\n",
      "     |      >>> msg.content = 'hello!'\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> msg = Message(role='user', content='hello!')\n",
      "     |      >>> 'content' in msg\n",
      "     |      True\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      False\n",
      "     |      >>> msg['tool_calls'] = []\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> msg['tool_calls'] = None\n",
      "     |      >>> 'tool_calls' in msg\n",
      "     |      True\n",
      "     |      >>> tool = Tool()\n",
      "     |      >>> 'type' in tool\n",
      "     |      True\n",
      "     |\n",
      "     |  __getitem__(self, key: str) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role']\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['nonexistent']\n",
      "     |      Traceback (most recent call last):\n",
      "     |      KeyError: 'nonexistent'\n",
      "     |\n",
      "     |  __setitem__(self, key: str, value: Any) -> None\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg['role'] = 'assistant'\n",
      "     |      >>> msg['role']\n",
      "     |      'assistant'\n",
      "     |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      "     |      >>> msg = Message(role='user', content='hello')\n",
      "     |      >>> msg['tool_calls'] = [tool_call]\n",
      "     |      >>> msg['tool_calls'][0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  get(self, key: str, default: Any = None) -> Any\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('role')\n",
      "     |      'user'\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent')\n",
      "     |      >>> msg = Message(role='user')\n",
      "     |      >>> msg.get('nonexistent', 'default')\n",
      "     |      'default'\n",
      "     |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      "     |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      "     |      'foo'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SubscriptableBaseModel:\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __copy__(self) -> 'Self'\n",
      "     |      Returns a shallow copy of the model.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      "     |      Returns a deep copy of the model.\n",
      "     |\n",
      "     |  __delattr__(self, item: 'str') -> 'Any'\n",
      "     |      Implement delattr(self, name).\n",
      "     |\n",
      "     |  __eq__(self, other: 'Any') -> 'bool'\n",
      "     |      Return self==value.\n",
      "     |\n",
      "     |  __getattr__(self, item: 'str') -> 'Any'\n",
      "     |\n",
      "     |  __getstate__(self) -> 'dict[Any, Any]'\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, /, **data: 'Any') -> 'None'\n",
      "     |      Create a new model by parsing and validating input data from keyword arguments.\n",
      "     |\n",
      "     |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "     |      validated to form a valid model.\n",
      "     |\n",
      "     |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      "     |\n",
      "     |  __iter__(self) -> 'TupleGenerator'\n",
      "     |      So `dict(model)` works.\n",
      "     |\n",
      "     |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]'\n",
      "     |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      "     |\n",
      "     |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      "     |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      "     |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      "     |\n",
      "     |  __repr__(self) -> 'str'\n",
      "     |      Return repr(self).\n",
      "     |\n",
      "     |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      "     |\n",
      "     |  __repr_name__(self) -> 'str'\n",
      "     |      Name of the instance's class, used in __repr__.\n",
      "     |\n",
      "     |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      "     |      Returns the string representation of a recursive object.\n",
      "     |\n",
      "     |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      "     |\n",
      "     |  __rich_repr__(self) -> 'RichReprResult'\n",
      "     |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      "     |\n",
      "     |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |\n",
      "     |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      "     |\n",
      "     |  __str__(self) -> 'str'\n",
      "     |      Return str(self).\n",
      "     |\n",
      "     |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      !!! warning \"Deprecated\"\n",
      "     |          This method is now deprecated; use `model_copy` instead.\n",
      "     |\n",
      "     |      If you need `include` or `exclude`, use:\n",
      "     |\n",
      "     |      ```python {test=\"skip\" lint=\"skip\"}\n",
      "     |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      "     |      data = {**data, **(update or {})}\n",
      "     |      copied = self.model_validate(data)\n",
      "     |      ```\n",
      "     |\n",
      "     |      Args:\n",
      "     |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      "     |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      "     |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      "     |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A copy of the model with included, excluded and updated fields as specified.\n",
      "     |\n",
      "     |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      "     |\n",
      "     |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      "     |\n",
      "     |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n",
      "     |\n",
      "     |      Returns a copy of the model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          update: Values to change/add in the new model. Note: the data is not validated\n",
      "     |              before creating the new model. You should trust this data.\n",
      "     |          deep: Set to `True` to make a deep copy of the model.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          New model instance.\n",
      "     |\n",
      "     |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n",
      "     |\n",
      "     |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          mode: The mode in which `to_python` should run.\n",
      "     |              If mode is 'json', the output will only contain JSON serializable types.\n",
      "     |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      "     |          include: A set of fields to include in the output.\n",
      "     |          exclude: A set of fields to exclude from the output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary representation of the model.\n",
      "     |\n",
      "     |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n",
      "     |\n",
      "     |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      "     |          include: Field(s) to include in the JSON output.\n",
      "     |          exclude: Field(s) to exclude from the JSON output.\n",
      "     |          context: Additional context to pass to the serializer.\n",
      "     |          by_alias: Whether to serialize using field aliases.\n",
      "     |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      "     |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      "     |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      "     |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      "     |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      "     |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      "     |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON string representation of the model.\n",
      "     |\n",
      "     |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      "     |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      "     |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's CoreSchema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          source: The class we are generating a schema for.\n",
      "     |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      "     |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A `pydantic-core` `CoreSchema`.\n",
      "     |\n",
      "     |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Hook into generating the model's JSON schema.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          core_schema: A `pydantic-core` CoreSchema.\n",
      "     |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      "     |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      "     |              or just call the handler with the original schema.\n",
      "     |          handler: Call into Pydantic's internal JSON schema generation.\n",
      "     |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      "     |              generation fails.\n",
      "     |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      "     |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      "     |              for a type.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A JSON schema, as a Python object.\n",
      "     |\n",
      "     |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      "     |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      "     |      be present when this is called.\n",
      "     |\n",
      "     |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      "     |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      "     |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      "     |\n",
      "     |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      "     |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      "     |              by pydantic.\n",
      "     |\n",
      "     |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Creates a new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      "     |      Default values are respected, but no other validation is performed.\n",
      "     |\n",
      "     |      !!! note\n",
      "     |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      "     |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      "     |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      "     |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      "     |          an error if extra values are passed, but they will be ignored.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      "     |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      "     |              Otherwise, the field names from the `values` argument will be used.\n",
      "     |          values: Trusted or pre-validated data dictionary.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A new instance of the `Model` class with validated data.\n",
      "     |\n",
      "     |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Generates a JSON schema for a model class.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          by_alias: Whether to use attribute aliases or not.\n",
      "     |          ref_template: The reference template.\n",
      "     |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      "     |              `GenerateJsonSchema` with your desired modifications\n",
      "     |          mode: The mode in which to generate the schema.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The JSON schema for the given model class.\n",
      "     |\n",
      "     |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Compute the class name for parametrizations of generic classes.\n",
      "     |\n",
      "     |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          params: Tuple of types of the class. Given a generic class\n",
      "     |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      "     |              the value `(str, int)` would be passed to `params`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      "     |\n",
      "     |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Try to rebuild the pydantic-core schema for the model.\n",
      "     |\n",
      "     |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      "     |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      "     |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      "     |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      "     |          _types_namespace: The types namespace, defaults to `None`.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      "     |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      "     |\n",
      "     |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate a pydantic model instance.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          from_attributes: Whether to extract data from object attributes.\n",
      "     |          context: Additional context to pass to the validator.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If the object could not be validated.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated model instance.\n",
      "     |\n",
      "     |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n",
      "     |\n",
      "     |      Validate the given JSON data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          json_data: The JSON data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |      Raises:\n",
      "     |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      "     |\n",
      "     |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |      Validate the given object with string data against the Pydantic model.\n",
      "     |\n",
      "     |      Args:\n",
      "     |          obj: The object containing string data to validate.\n",
      "     |          strict: Whether to enforce types strictly.\n",
      "     |          context: Extra variables to pass to the validator.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          The validated Pydantic model.\n",
      "     |\n",
      "     |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __fields_set__\n",
      "     |\n",
      "     |  model_computed_fields\n",
      "     |      Get metadata about the computed fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n",
      "     |\n",
      "     |  model_extra\n",
      "     |      Get extra fields set during validation.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      "     |\n",
      "     |  model_fields\n",
      "     |      Get metadata about the fields defined on the model.\n",
      "     |\n",
      "     |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n",
      "     |      In V3, this property will be removed from the `BaseModel` class.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n",
      "     |\n",
      "     |  model_fields_set\n",
      "     |      Returns the set of fields that have been explicitly set on this model instance.\n",
      "     |\n",
      "     |      Returns:\n",
      "     |          A set of strings representing the fields that have been set,\n",
      "     |              i.e. that were not filled from defaults.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __pydantic_extra__\n",
      "     |\n",
      "     |  __pydantic_fields_set__\n",
      "     |\n",
      "     |  __pydantic_private__\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      "     |\n",
      "     |  __hash__ = None\n",
      "     |\n",
      "     |  __pydantic_root_model__ = False\n",
      "\n",
      "DATA\n",
      "    __all__ = ['AsyncClient', 'ChatResponse', 'Client', 'EmbedResponse', '...\n",
      "\n",
      "FILE\n",
      "    /Users/arda/Documents/llama-finetune/myenv/lib/python3.12/site-packages/ollama/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ollama api and keep it running.\n",
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create models dir\n",
    "!mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model file \n",
    "!nano models/Modelfile FROM ../sql_agent/ggml-model-f16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 0% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 0% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 1% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 2% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 4% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 5% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 7% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 8% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 9% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 10% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 12% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 13% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 15% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 16% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 17% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 18% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 20% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 21% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 23% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 24% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 25% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 26% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 28% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 29% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 30% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 32% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 33% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 34% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 35% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 37% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 38% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 40% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 41% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 43% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 44% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 45% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 46% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 48% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 49% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 50% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 52% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 54% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 55% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 56% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 58% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 59% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 60% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 62% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 63% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 64% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 66% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 67% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 69% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 71% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 71% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 73% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 75% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 76% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 77% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 79% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 80% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 82% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 84% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 85% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 87% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 88% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 89% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 91% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 93% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 95% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 96% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 98% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 98% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:0b11f5e08637132ab4728ae57004d273db3b9b089045d4846ab066513571746b \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "#Create Ollama model\n",
    "!ollama create sql-agent -f models/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED      \n",
      "sql-agent:latest    c6b495001637    6.4 GB    7 minutes ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama run sql-agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
